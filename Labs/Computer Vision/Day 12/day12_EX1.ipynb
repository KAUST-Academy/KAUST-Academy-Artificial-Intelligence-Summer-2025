{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Vision Transformer (ViT) Exercise\n",
        "###Welcome to this hands-on exercise on Vision Transformers! You'll build a ViT from scratch to classify images in the CIFAR-10 dataset, learning how the revolutionary Transformer architecture works for computer vision tasks.\n",
        "##Learning Objectives\n",
        "\n",
        "* Understand the Transformer architecture and self-attention mechanism\n",
        "* Learn how to adapt Transformers for computer vision (Vision Transformers)\n",
        "* Implement patch embedding and positional encoding for images\n",
        "* Build multi-head attention and feed-forward components\n",
        "* Master the complete ViT pipeline from patches to predictions\n",
        "* Compare ViT performance with CNNs\n",
        "\n",
        "##Why Transformers for Vision?\n",
        "###Traditional CNNs use local receptive fields, but Transformers can:\n",
        "\n",
        "* Global Attention: Every patch can attend to every other patch\n",
        "* No Inductive Bias: Learn spatial relationships from data\n",
        "* Scalability: Performance improves with larger datasets\n",
        "* Unified Architecture: Same architecture for NLP and Vision\n",
        "\n",
        "#Architecture: Image → Patches → Embeddings → Transformer → Classification\n"
      ],
      "metadata": {
        "id": "sPXksRDVqcQP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Introduction to Vision Transformers\n",
        "###Key Concepts:\n",
        "\n",
        "* Patch Embedding: Split image into patches and embed them\n",
        "* Positional Encoding: Add spatial information to patches\n",
        "* Self-Attention: Let patches \"talk\" to each other\n",
        "* Transformer Encoder: Stack of attention + feed-forward layers\n",
        "* Classification Token: Special [CLS] token for final prediction\n",
        "\n",
        "###CIFAR-10 Dataset:\n",
        "\n",
        "* 60,000 32x32 color images\n",
        "* 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck\n",
        "* 50,000 training + 10,000 test images"
      ],
      "metadata": {
        "id": "TJOF43PiiQuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import math\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device and random seeds\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# CIFAR-10 class names\n",
        "CIFAR10_CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                   'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "u8pufardqu7A",
        "outputId": "2849f97b-d4e7-4929-a085-df65ab591539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'transforms' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-2862384917.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m transform = transforms.Compose([\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# TODO: Add normalization transform to scale pixel values to [-1, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Hint: Use transforms.Normalize with mean=0.5, std=0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# YOUR CODE HERE:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Loading and Exploration"
      ],
      "metadata": {
        "id": "HQim7acyUn3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_cifar10_data():\n",
        "    \"\"\"Load and explore CIFAR-10 dataset\"\"\"\n",
        "\n",
        "    # TODO: Define data transforms\n",
        "    # For ViT, we typically need:\n",
        "    # 1. Convert to tensor\n",
        "    # 2. Normalize (ImageNet stats work well for transfer learning)\n",
        "    # 3. Optional: data augmentation\n",
        "    # YOUR CODE HERE:\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "        # Add data augmentation for training\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # TODO: Load CIFAR-10 datasets\n",
        "    # YOUR CODE HERE:\n",
        "    train_dataset = CIFAR10(root='/content/', train=True, download=True, transform=transform_train)\n",
        "    test_dataset = CIFAR10(root='/content/', train=False, download=True, transform=transform_test)\n",
        "\n",
        "    # Split training set into train and validation\n",
        "    train_size = int(0.9 * len(train_dataset))\n",
        "    val_size = len(train_dataset) - train_size\n",
        "    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "    # Update validation dataset transform to test transform\n",
        "    val_dataset.dataset = CIFAR10(root='./data', train=True, download=False, transform=transform_test)\n",
        "\n",
        "    print(f\"Training samples: {len(train_dataset)}\")\n",
        "    print(f\"Validation samples: {len(val_dataset)}\")\n",
        "    print(f\"Test samples: {len(test_dataset)}\")\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "def visualize_cifar10_samples(dataset, num_samples=20):\n",
        "    \"\"\"Visualize sample images from CIFAR-10\"\"\"\n",
        "\n",
        "    # Create a temporary dataset without normalization for visualization\n",
        "    viz_transform = transforms.Compose([transforms.ToTensor()])\n",
        "    viz_dataset = CIFAR10(root='./data', train=True, download=False, transform=viz_transform)\n",
        "\n",
        "    # Get random samples\n",
        "    indices = np.random.choice(len(viz_dataset), num_samples, replace=False)\n",
        "\n",
        "    fig, axes = plt.subplots(4, 5, figsize=(12, 10))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        image, label = viz_dataset[idx]\n",
        "\n",
        "        # Convert tensor to numpy and transpose for plotting\n",
        "        image_np = image.permute(1, 2, 0).numpy()\n",
        "\n",
        "        axes[i].imshow(image_np)\n",
        "        axes[i].set_title(f'{CIFAR10_CLASSES[label]}')\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.suptitle('CIFAR-10 Sample Images')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_class_distribution(dataset):\n",
        "    \"\"\"Analyze the distribution of classes in the dataset\"\"\"\n",
        "\n",
        "    # Count classes\n",
        "    class_counts = [0] * 10\n",
        "\n",
        "    # For subset datasets, we need to access the underlying dataset\n",
        "    if hasattr(dataset, 'dataset'):\n",
        "        base_dataset = dataset.dataset\n",
        "        indices = dataset.indices\n",
        "        for idx in indices:\n",
        "            _, label = base_dataset[idx]\n",
        "            class_counts[label] += 1\n",
        "    else:\n",
        "        for _, label in dataset:\n",
        "            class_counts[label] += 1\n",
        "\n",
        "    # Plot distribution\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    bars = plt.bar(CIFAR10_CLASSES, class_counts, color='skyblue', alpha=0.7)\n",
        "    plt.title('CIFAR-10 Class Distribution')\n",
        "    plt.xlabel('Classes')\n",
        "    plt.ylabel('Number of Images')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, count in zip(bars, class_counts):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10,\n",
        "                str(count), ha='center', va='bottom')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Class distribution:\")\n",
        "    for i, (class_name, count) in enumerate(zip(CIFAR10_CLASSES, class_counts)):\n",
        "        print(f\"{i}: {class_name}: {count} images\")\n",
        "\n",
        "# Load and explore the data\n",
        "train_dataset, val_dataset, test_dataset = load_cifar10_data()\n",
        "\n",
        "# Visualize samples\n",
        "visualize_cifar10_samples(train_dataset)\n",
        "\n",
        "# Analyze class distribution\n",
        "analyze_class_distribution(train_dataset)"
      ],
      "metadata": {
        "id": "FkfbB6TghotS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Patch Embedding - Converting Images to Sequences"
      ],
      "metadata": {
        "id": "Y4dyLHubUtKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"Convert image into sequence of patch embeddings\"\"\"\n",
        "\n",
        "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=192):\n",
        "        super(PatchEmbedding, self).__init__()\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.in_channels = in_channels\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # TODO: Calculate number of patches\n",
        "        # YOUR CODE HERE:\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        # TODO: Create patch embedding layer\n",
        "        # Hint: Use Conv2d with kernel_size=patch_size, stride=patch_size\n",
        "        # This effectively splits the image into patches and projects them\n",
        "        # YOUR CODE HERE:\n",
        "        self.projection = nn.Conv2d(\n",
        "            in_channels,\n",
        "            embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Implement patch embedding\n",
        "        # Steps:\n",
        "        # 1. Apply convolution to create patches\n",
        "        # 2. Flatten spatial dimensions\n",
        "        # 3. Transpose to get (batch_size, num_patches, embed_dim)\n",
        "        # YOUR CODE HERE:\n",
        "\n",
        "        # x shape: (batch_size, channels, height, width)\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # Apply patch embedding convolution\n",
        "        # Output shape: (batch_size, embed_dim, num_patches_h, num_patches_w)\n",
        "        x = self.projection(x)\n",
        "\n",
        "        # Flatten spatial dimensions and transpose\n",
        "        # Shape: (batch_size, embed_dim, num_patches_h * num_patches_w)\n",
        "        x = x.flatten(2)\n",
        "\n",
        "        # Transpose to get (batch_size, num_patches, embed_dim)\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Test patch embedding\n",
        "patch_embed = PatchEmbedding(img_size=32, patch_size=4, in_channels=3, embed_dim=192)\n",
        "\n",
        "# Test with a batch of images\n",
        "test_input = torch.randn(8, 3, 32, 32)  # Batch of 8 CIFAR-10 images\n",
        "patch_output = patch_embed(test_input)\n",
        "\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "print(f\"Patch embedding output shape: {patch_output.shape}\")\n",
        "print(f\"Number of patches: {patch_embed.num_patches}\")\n",
        "print(f\"Each patch is embedded to {patch_embed.embed_dim} dimensions\")\n",
        "\n",
        "# Visualize patch splitting\n",
        "def visualize_patch_splitting():\n",
        "    \"\"\"Visualize how an image is split into patches\"\"\"\n",
        "\n",
        "    # Get a sample image\n",
        "    viz_transform = transforms.Compose([transforms.ToTensor()])\n",
        "    viz_dataset = CIFAR10(root='./data', train=True, download=False, transform=viz_transform)\n",
        "    sample_image, label = viz_dataset[0]\n",
        "\n",
        "    patch_size = 4\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # Original image\n",
        "    axes[0].imshow(sample_image.permute(1, 2, 0))\n",
        "    axes[0].set_title(f'Original Image: {CIFAR10_CLASSES[label]}')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Image with patch grid\n",
        "    axes[1].imshow(sample_image.permute(1, 2, 0))\n",
        "\n",
        "    # Draw patch boundaries\n",
        "    for i in range(0, 32, patch_size):\n",
        "        axes[1].axhline(y=i, color='red', linewidth=1)\n",
        "        axes[1].axvline(x=i, color='red', linewidth=1)\n",
        "\n",
        "    axes[1].set_title(f'Image with {patch_size}x{patch_size} Patch Grid')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    # Individual patches\n",
        "    patches = []\n",
        "    for i in range(0, 32, patch_size):\n",
        "        for j in range(0, 32, patch_size):\n",
        "            patch = sample_image[:, i:i+patch_size, j:j+patch_size]\n",
        "            patches.append(patch)\n",
        "\n",
        "    # Show first 16 patches\n",
        "    patch_grid = torch.stack(patches[:16])\n",
        "    grid_image = torchvision.utils.make_grid(patch_grid, nrow=4, padding=1)\n",
        "\n",
        "    axes[2].imshow(grid_image.permute(1, 2, 0))\n",
        "    axes[2].set_title('First 16 Patches (4x4 grid)')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_patch_splitting()"
      ],
      "metadata": {
        "id": "Rj4WBvG-sIPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encoding - Adding Spatial Information"
      ],
      "metadata": {
        "id": "rGHwN1N6UyHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Add positional information to patch embeddings\"\"\"\n",
        "\n",
        "    def __init__(self, num_patches, embed_dim, dropout=0.1):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        self.num_patches = num_patches\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # TODO: Create learnable positional embeddings\n",
        "        # Option 1: Learnable embeddings (most common in ViT)\n",
        "        # Option 2: Fixed sinusoidal embeddings (original Transformer)\n",
        "        # YOUR CODE HERE:\n",
        "\n",
        "        # Learnable positional embeddings\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
        "\n",
        "        # Classification token\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Add positional encoding\n",
        "        # Steps:\n",
        "        # 1. Add classification token to the beginning\n",
        "        # 2. Add positional embeddings\n",
        "        # 3. Apply dropout\n",
        "        # YOUR CODE HERE:\n",
        "\n",
        "        batch_size, num_patches, embed_dim = x.shape\n",
        "\n",
        "        # Expand classification token for batch\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "\n",
        "        # Concatenate cls token with patch embeddings\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "\n",
        "        # Add positional embeddings\n",
        "        x = x + self.pos_embedding\n",
        "\n",
        "        # Apply dropout\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Alternative: Fixed sinusoidal positional encoding\n",
        "def get_sinusoidal_positional_encoding(num_patches, embed_dim):\n",
        "    \"\"\"Create fixed sinusoidal positional encodings\"\"\"\n",
        "\n",
        "    # TODO: Implement sinusoidal positional encoding\n",
        "    # This is the original approach from \"Attention is All You Need\"\n",
        "    # YOUR CODE HERE:\n",
        "\n",
        "    position = torch.arange(num_patches).unsqueeze(1).float()\n",
        "    div_term = torch.exp(torch.arange(0, embed_dim, 2).float() *\n",
        "                        -(math.log(10000.0) / embed_dim))\n",
        "\n",
        "    pos_encoding = torch.zeros(num_patches, embed_dim)\n",
        "    pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "    pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    return pos_encoding.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Test positional encoding\n",
        "num_patches = 64  # 8x8 patches for 32x32 image with 4x4 patches\n",
        "embed_dim = 192\n",
        "\n",
        "pos_encoder = PositionalEncoding(num_patches, embed_dim)\n",
        "\n",
        "# Test input (batch_size=8, num_patches=64, embed_dim=192)\n",
        "test_patches = torch.randn(8, num_patches, embed_dim)\n",
        "encoded_patches = pos_encoder(test_patches)\n",
        "\n",
        "print(f\"Input patches shape: {test_patches.shape}\")\n",
        "print(f\"After positional encoding: {encoded_patches.shape}\")\n",
        "print(\"Note: sequence length increased by 1 due to classification token\")\n",
        "\n",
        "# Visualize positional embeddings\n",
        "def visualize_positional_embeddings():\n",
        "    \"\"\"Visualize the learned positional embeddings\"\"\"\n",
        "\n",
        "    pos_embeddings = pos_encoder.pos_embedding[0, 1:, :].detach()  # Exclude cls token\n",
        "\n",
        "    # Reshape to spatial layout (8x8 patches)\n",
        "    spatial_size = int(math.sqrt(num_patches))\n",
        "    pos_reshaped = pos_embeddings.view(spatial_size, spatial_size, embed_dim)\n",
        "\n",
        "    # Plot first few dimensions of positional embeddings\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    for i in range(8):\n",
        "        im = axes[i].imshow(pos_reshaped[:, :, i], cmap='viridis')\n",
        "        axes[i].set_title(f'Position Embedding Dim {i}')\n",
        "        axes[i].axis('off')\n",
        "        plt.colorbar(im, ax=axes[i])\n",
        "\n",
        "    plt.suptitle('Learned Positional Embeddings (First 8 Dimensions)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Uncomment after training to visualize learned embeddings\n",
        "# visualize_positional_embeddings()"
      ],
      "metadata": {
        "id": "nE2aR_JpsvOh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "fba179f9-c1df-42a8-c3a4-8918109fc3e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-9-1062803426.py, line 7)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-9-1062803426.py\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    generator =\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Head Self-Attention - The Heart of Transformers"
      ],
      "metadata": {
        "id": "4ESv0w4mU2TK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    \"\"\"Multi-head self-attention mechanism\"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "\n",
        "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        # TODO: Define linear projections for Q, K, V\n",
        "        # YOUR CODE HERE:\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Implement multi-head self-attention\n",
        "        # Steps:\n",
        "        # 1. Create Q, K, V from input\n",
        "        # 2. Reshape for multi-head attention\n",
        "        # 3. Compute attention scores\n",
        "        # 4. Apply softmax and dropout\n",
        "        # 5. Apply attention to values\n",
        "        # 6. Concatenate heads and project\n",
        "        # YOUR CODE HERE:\n",
        "\n",
        "        batch_size, seq_length, embed_dim = x.shape\n",
        "\n",
        "        # Generate Q, K, V\n",
        "        qkv = self.qkv(x)  # (batch_size, seq_length, embed_dim * 3)\n",
        "        qkv = qkv.reshape(batch_size, seq_length, 3, self.num_heads, self.head_dim)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, batch_size, num_heads, seq_length, head_dim)\n",
        "\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        # Compute attention scores\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        # Apply softmax\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Apply attention to values\n",
        "        out = torch.matmul(attn_weights, v)\n",
        "\n",
        "        # Concatenate heads\n",
        "        out = out.transpose(1, 2).reshape(batch_size, seq_length, embed_dim)\n",
        "\n",
        "        # Final projection\n",
        "        out = self.proj(out)\n",
        "\n",
        "        return out, attn_weights\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Feed-forward network (MLP) component\"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, hidden_dim, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "\n",
        "        # TODO: Define feed-forward network\n",
        "        # Typical structure: Linear -> GELU -> Dropout -> Linear\n",
        "        # YOUR CODE HERE:\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Single Transformer encoder block\"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, mlp_ratio=4, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        # TODO: Define transformer block components\n",
        "        # Structure: LayerNorm -> MultiHeadAttention -> Residual\n",
        "        #           LayerNorm -> FeedForward -> Residual\n",
        "        # YOUR CODE HERE:\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads, dropout)\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        hidden_dim = int(embed_dim * mlp_ratio)\n",
        "        self.mlp = FeedForward(embed_dim, hidden_dim, dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Implement transformer block forward pass\n",
        "        # Use residual connections around attention and MLP\n",
        "        # YOUR CODE HERE:\n",
        "\n",
        "        # Multi-head attention with residual connection\n",
        "        attn_out, attn_weights = self.attn(self.norm1(x))\n",
        "        x = x + attn_out\n",
        "\n",
        "        # Feed-forward with residual connection\n",
        "        mlp_out = self.mlp(self.norm2(x))\n",
        "        x = x + mlp_out\n",
        "\n",
        "        return x, attn_weights\n",
        "\n",
        "# Test transformer components\n",
        "embed_dim = 192\n",
        "num_heads = 8\n",
        "seq_length = 65  # 64 patches + 1 cls token\n",
        "\n",
        "# Test multi-head attention\n",
        "attention = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "test_input = torch.randn(4, seq_length, embed_dim)\n",
        "attn_output, attn_weights = attention(test_input)\n",
        "\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "print(f\"Attention output shape: {attn_output.shape}\")\n",
        "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
        "\n",
        "# Test transformer block\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads)\n",
        "block_output, block_attn_weights = transformer_block(test_input)\n",
        "\n",
        "print(f\"Transformer block output shape: {block_output.shape}\")"
      ],
      "metadata": {
        "id": "sGv-GULptAjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete Vision Transformer Model"
      ],
      "metadata": {
        "id": "G9muxdXDU6O_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\"Complete Vision Transformer model\"\"\"\n",
        "\n",
        "    def __init__(self, img_size=32, patch_size=4, in_channels=3, num_classes=10,\n",
        "                 embed_dim=192, depth=12, num_heads=8, mlp_ratio=4, dropout=0.1):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_classes = num_classes\n",
        "        self.embed_dim = embed_dim\n",
        "        self.depth = depth\n",
        "\n",
        "        # TODO: Assemble all ViT components\n",
        "        # 1. Patch embedding\n",
        "        # 2. Positional encoding\n",
        "        # 3. Stack of transformer blocks\n",
        "        # 4. Final layer norm\n",
        "        # 5. Classification head\n",
        "        # YOUR CODE HERE:\n",
        "\n",
        "        # Patch embedding\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        # Positional encoding (includes cls token)\n",
        "        self.pos_embed = PositionalEncoding(num_patches, embed_dim, dropout)\n",
        "\n",
        "        # Transformer encoder blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        # Final layer norm\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # Classification head\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        \"\"\"Initialize model weights\"\"\"\n",
        "        if isinstance(m, nn.Linear):\n",
        "            torch.nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Implement complete ViT forward pass\n",
        "        # Steps:\n",
        "        # 1. Patch embedding\n",
        "        # 2. Add positional encoding\n",
        "        # 3. Pass through transformer blocks\n",
        "        # 4. Extract cls token\n",
        "        # 5. Apply final norm and classification head\n",
        "        # YOUR CODE HERE:\n",
        "\n",
        "        # Patch embedding\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # Add positional encoding (includes cls token)\n",
        "        x = self.pos_embed(x)\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        attn_weights_list = []\n",
        "        for block in self.blocks:\n",
        "            x, attn_weights = block(x)\n",
        "            attn_weights_list.append(attn_weights)\n",
        "\n",
        "        # Apply final layer norm\n",
        "        x = self.norm(x)\n",
        "\n",
        "        # Extract cls token (first token) and classify\n",
        "        cls_token = x[:, 0]\n",
        "        logits = self.head(cls_token)\n",
        "\n",
        "        return logits, attn_weights_list\n",
        "\n",
        "    def get_attention_maps(self, x, layer_idx=-1):\n",
        "        \"\"\"Extract attention maps for visualization\"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            _, attn_weights_list = self.forward(x)\n",
        "\n",
        "            # Return attention weights from specified layer\n",
        "            return attn_weights_list[layer_idx]\n",
        "\n",
        "# Create different ViT configurations\n",
        "def create_vit_tiny():\n",
        "    \"\"\"Create a tiny ViT for CIFAR-10\"\"\"\n",
        "    return VisionTransformer(\n",
        "        img_size=32, patch_size=4, in_channels=3, num_classes=10,\n",
        "        embed_dim=192, depth=6, num_heads=8, mlp_ratio=4, dropout=0.1\n",
        "    )\n",
        "\n",
        "def create_vit_small():\n",
        "    \"\"\"Create a small ViT for CIFAR-10\"\"\"\n",
        "    return VisionTransformer(\n",
        "        img_size=32, patch_size=4, in_channels=3, num_classes=10,\n",
        "        embed_dim=384, depth=12, num_heads=8, mlp_ratio=4, dropout=0.1\n",
        "    )\n",
        "\n",
        "def create_vit_base():\n",
        "    \"\"\"Create a base ViT for CIFAR-10\"\"\"\n",
        "    return VisionTransformer(\n",
        "        img_size=32, patch_size=4, in_channels=3, num_classes=10,\n",
        "        embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, dropout=0.1\n",
        "    )\n",
        "\n",
        "# Test model creation\n",
        "model = create_vit_tiny()\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Test forward pass\n",
        "test_input = torch.randn(2, 3, 32, 32)\n",
        "logits, attn_weights = model(test_input)\n",
        "\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "print(f\"Output logits shape: {logits.shape}\")\n",
        "print(f\"Number of attention layers: {len(attn_weights)}\")\n",
        "print(f\"Attention weights shape (last layer): {attn_weights[-1].shape}\")\n",
        "\n",
        "# Model summary\n",
        "def count_parameters(model):\n",
        "    \"\"\"Count trainable parameters\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nModel Summary:\")\n",
        "print(f\"Trainable parameters: {count_parameters(model):,}\")\n",
        "\n",
        "# Compare model sizes\n",
        "models = {\n",
        "    'ViT-Tiny': create_vit_tiny(),\n",
        "    'ViT-Small': create_vit_small(),\n",
        "    'ViT-Base': create_vit_base()\n",
        "}\n",
        "\n",
        "print(f\"\\nModel Size Comparison:\")\n",
        "for name, model in models.items():\n",
        "    params = count_parameters(model)\n",
        "    print(f\"{name}: {params:,} parameters\")"
      ],
      "metadata": {
        "id": "he43WhO7tBIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Setup and Data Loaders"
      ],
      "metadata": {
        "id": "rsxkK1LyU9yE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_loaders(train_dataset, val_dataset, test_dataset, batch_size=128):\n",
        "    \"\"\"Create data loaders for training\"\"\"\n",
        "\n",
        "    # TODO: Create data loaders with appropriate batch sizes\n",
        "    # YOUR CODE HERE:\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "def train_vit(model, train_loader, val_loader, num_epochs=100,\n",
        "              learning_rate=3e-4, weight_decay=0.1):\n",
        "    \"\"\"Train the Vision Transformer\"\"\"\n",
        "\n",
        "    # TODO: Setup training components\n",
        "    # ViTs typically use AdamW optimizer with cosine learning rate schedule\n",
        "    # YOUR CODE HERE:\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    # Cosine annealing scheduler\n",
        "    scheduler = optim.lr_"
      ],
      "metadata": {
        "id": "OICCuPLPtO8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Made by Abdullah Jan"
      ],
      "metadata": {
        "id": "t7L75dzWVU7H"
      }
    }
  ]
}