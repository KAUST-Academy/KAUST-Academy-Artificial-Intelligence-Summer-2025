{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "77def1b0",
      "metadata": {},
      "source": [
        "![image.png](https://i.imgur.com/a3uAqnb.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f15a624",
      "metadata": {},
      "source": [
        "# Fine-tuning Vision Transformer for Animal Classification: Animals10 Dataset\n",
        "- **Images**: Animal photos from 10 different classes **(224x224, RGB)**\n",
        "- **Classes**: 10 animal classes (dog, cat, horse, etc.) with Italian names translated to English\n",
        "- **Task**: Multi-class image classification using pre-trained ViT\n",
        "- **Model**: Google's ViT-Large-Patch16-224 fine-tuned for animal recognition\n",
        "\n",
        "![Animals10 Overview](https://i.imgur.com/IS4NeFZ.png)\n",
        "\n",
        "This notebook is heavily inspired by this [notebook](https://huggingface.co/learn/cookbook/en/fine_tuning_vit_custom_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0edf864",
      "metadata": {},
      "source": [
        "## üéØ **What is Vision Transformer (ViT)?**\n",
        "Vision Transformer treats images as sequences of patches, similar to how text transformers process word tokens:\n",
        "\n",
        "1Ô∏è‚É£ **Patch Embedding** ‚Üí Split image into 16x16 patches and embed them  \n",
        "2Ô∏è‚É£ **Position Encoding** ‚Üí Add positional information to patches  \n",
        "3Ô∏è‚É£ **Transformer Encoder** ‚Üí Process patch sequences with self-attention  \n",
        "4Ô∏è‚É£ **Classification Head** ‚Üí Final layer for class prediction  \n",
        "\n",
        "Unlike CNNs, ViT uses **attention mechanisms** to understand spatial relationships between image patches.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1a731ad",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£ **Dataset Setup & Preprocessing**\n",
        "\n",
        "### Installing Dependencies and Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b03271a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Essential libraries for ViT fine-tuning\n",
        "import kagglehub  # For downloading Kaggle datasets\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Hugging Face ecosystem\n",
        "from datasets import Dataset, DatasetDict, load_dataset\n",
        "from transformers import ViTImageProcessor, ViTForImageClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# PyTorch for deep learning\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import (\n",
        "    CenterCrop, Compose, Normalize, RandomHorizontalFlip, \n",
        "    RandomResizedCrop, ToTensor, Resize, RandomRotation, ColorJitter\n",
        ")\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0924b09d",
      "metadata": {},
      "source": [
        "### Downloading the Animals10 Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97000808",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîπ Download the Animals10 dataset from Kaggle\n",
        "print(\"Downloading Animals10 dataset...\")\n",
        "dataset_path = kagglehub.dataset_download(\"alessiocorrado99/animals10\")\n",
        "print(\"Path to dataset files:\", dataset_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2cd5730",
      "metadata": {},
      "source": [
        "### Class Label Translation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc39a742",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîπ Italian to English translation mapping for animal classes\n",
        "# The Animals10 dataset uses Italian class names, so we translate them\n",
        "translate = {\n",
        "    \"cane\": \"dog\", \n",
        "    \"cavallo\": \"horse\", \n",
        "    \"elefante\": \"elephant\", \n",
        "    \"farfalla\": \"butterfly\", \n",
        "    \"gallina\": \"chicken\",\n",
        "    \"gatto\": \"cat\", \n",
        "    \"mucca\": \"cow\", \n",
        "    \"pecora\": \"sheep\", \n",
        "    \"ragno\": \"spider\", \n",
        "    \"scoiattolo\": \"squirrel\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92a43de4",
      "metadata": {},
      "source": [
        "## 2Ô∏è‚É£ **Dataset Organization & Memory Management**\n",
        "\n",
        "### Dataset Organization Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e23e8a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def organize_animals10_dataset(dataset_path, max_total_images=5000):\n",
        "    \"\"\"\n",
        "    üîπ Organize the Animals10 dataset into train/validation/test splits\n",
        "    \n",
        "    Args:\n",
        "        dataset_path: Path to the downloaded dataset\n",
        "        max_total_images: Limit total images to reduce memory usage (important for ViT!)\n",
        "    \n",
        "    Returns:\n",
        "        images: List of image file paths\n",
        "        labels: List of corresponding class labels (integers)\n",
        "        id2label: Dictionary mapping class IDs to English names\n",
        "        label2id: Dictionary mapping English names to class IDs\n",
        "    \"\"\"\n",
        "    raw_img_path = os.path.join(dataset_path, \"raw-img\")\n",
        "    \n",
        "    if not os.path.exists(raw_img_path):\n",
        "        raise FileNotFoundError(f\"raw-img folder not found in {dataset_path}\")\n",
        "    \n",
        "    # üîπ Get all animal classes (folder names)\n",
        "    animal_classes = [d for d in os.listdir(raw_img_path) \n",
        "                     if os.path.isdir(os.path.join(raw_img_path, d))]\n",
        "    \n",
        "    print(f\"Found {len(animal_classes)} animal classes: {animal_classes}\")\n",
        "    print(f\"Limiting dataset to maximum {max_total_images} images for memory efficiency\")\n",
        "    \n",
        "    # üîπ Create label mappings for the model\n",
        "    id2label = {i: translate.get(cls, cls) for i, cls in enumerate(animal_classes)}\n",
        "    label2id = {label: i for i, label in id2label.items()}\n",
        "    \n",
        "    # üîπ Calculate images per class to reach the target\n",
        "    images_per_class = max_total_images // len(animal_classes)\n",
        "    \n",
        "    # üîπ Collect limited images and labels\n",
        "    images = []\n",
        "    labels = []\n",
        "    \n",
        "    for class_idx, class_name in enumerate(animal_classes):\n",
        "        class_path = os.path.join(raw_img_path, class_name)\n",
        "        image_files = [f for f in os.listdir(class_path) \n",
        "                      if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "        \n",
        "        # üîπ Limit images per class for memory management\n",
        "        selected_images = image_files[:images_per_class]\n",
        "        \n",
        "        print(f\"Class '{class_name}' ({translate.get(class_name, class_name)}): \"\n",
        "              f\"Using {len(selected_images)} out of {len(image_files)} images\")\n",
        "        \n",
        "        for img_file in selected_images:\n",
        "            img_path = os.path.join(class_path, img_file)\n",
        "            images.append(img_path)\n",
        "            labels.append(class_idx)\n",
        "    \n",
        "    print(f\"\\nTotal selected images: {len(images)}\")\n",
        "    return images, labels, id2label, label2id"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebdb52c0",
      "metadata": {},
      "source": [
        "### Hugging Face Dataset Creation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57a03a1f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_hf_dataset(images, labels, id2label):\n",
        "    \"\"\"\n",
        "    üîπ Create a Hugging Face dataset from image paths and labels\n",
        "    \n",
        "    This function:\n",
        "    - Loads images in batches to manage memory usage\n",
        "    - Resizes images to 224x224 (ViT input size)\n",
        "    - Creates proper dataset features for stratified splitting\n",
        "    \n",
        "    Args:\n",
        "        images: List of image file paths\n",
        "        labels: List of class labels\n",
        "        id2label: Class ID to name mapping\n",
        "    \n",
        "    Returns:\n",
        "        Hugging Face Dataset object ready for training\n",
        "    \"\"\"\n",
        "    from datasets import Features, Image as HFImage, ClassLabel\n",
        "    \n",
        "    print(f\"Loading {len(images)} images into dataset...\")\n",
        "    \n",
        "    loaded_images = []\n",
        "    valid_labels = []\n",
        "    \n",
        "    # üîπ Process images in smaller batches to avoid memory issues\n",
        "    batch_size = 100\n",
        "    for i in range(0, len(images), batch_size):\n",
        "        print(f\"Processing batch {i//batch_size + 1}/{(len(images)-1)//batch_size + 1}\")\n",
        "        \n",
        "        batch_images = images[i:i+batch_size]\n",
        "        batch_labels = labels[i:i+batch_size]\n",
        "        \n",
        "        for img_path, label in zip(batch_images, batch_labels):\n",
        "            try:\n",
        "                # üîπ Load and preprocess image\n",
        "                image = Image.open(img_path).convert('RGB')  # Ensure RGB format\n",
        "                # Resize to ViT input size to reduce memory footprint\n",
        "                image = image.resize((224, 224), Image.Resampling.LANCZOS)\n",
        "                loaded_images.append(image)\n",
        "                valid_labels.append(label)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {img_path}: {e}\")\n",
        "    \n",
        "    print(f\"Successfully loaded {len(loaded_images)} images\")\n",
        "    \n",
        "    # üîπ Define proper features with ClassLabel for stratification\n",
        "    features = Features({\n",
        "        'image': HFImage(),\n",
        "        'label': ClassLabel(names=list(id2label.values()))\n",
        "    })\n",
        "    \n",
        "    return Dataset.from_dict({\n",
        "        'image': loaded_images,\n",
        "        'label': valid_labels\n",
        "    }, features=features)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "347a88c9",
      "metadata": {},
      "source": [
        "### Execute Dataset Organization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2eb6468",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîπ Organize the dataset with memory limitation\n",
        "# Note: ViT models require more memory than CNNs, so we limit dataset size\n",
        "images, labels, id2label, label2id = organize_animals10_dataset(dataset_path, max_total_images=5000)\n",
        "\n",
        "print(f\"\\nDataset Summary:\")\n",
        "print(f\"Total images: {len(images)}\")\n",
        "print(f\"Number of classes: {len(id2label)}\")\n",
        "print(f\"Label mapping: {id2label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6e6280f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîπ Create the dataset with proper ClassLabel features\n",
        "# This step loads all images into memory - may take a few minutes\n",
        "full_dataset = create_hf_dataset(images, labels, id2label)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e0e5dcc",
      "metadata": {},
      "source": [
        "## 3Ô∏è‚É£ **Train/Validation/Test Split**\n",
        "\n",
        "### Stratified Dataset Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3758391c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîπ Split into train/validation/test (70/15/15) with stratification\n",
        "# Stratification ensures balanced class distribution across splits\n",
        "train_val_split = full_dataset.train_test_split(test_size=0.3, stratify_by_column='label')\n",
        "val_test_split = train_val_split['test'].train_test_split(test_size=0.5, stratify_by_column='label')\n",
        "\n",
        "# üîπ Create final dataset dictionary\n",
        "dataset = DatasetDict({\n",
        "    'train': train_val_split['train'],        # 70% for training\n",
        "    'validation': val_test_split['train'],    # 15% for validation\n",
        "    'test': val_test_split['test']           # 15% for final testing\n",
        "})\n",
        "\n",
        "print(f\"\\nDataset splits:\")\n",
        "print(f\"Train: {len(dataset['train'])} images\")\n",
        "print(f\"Validation: {len(dataset['validation'])} images\") \n",
        "print(f\"Test: {len(dataset['test'])} images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df506f93",
      "metadata": {},
      "source": [
        "## 4Ô∏è‚É£ **Dataset Visualization**\n",
        "\n",
        "### Sample Visualization Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaf67081",
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_samples(dataset, id2label, num_classes=10):\n",
        "    \"\"\"\n",
        "    üîπ Visualize one sample from each class to understand the dataset\n",
        "    \n",
        "    This helps us verify:\n",
        "    - Image quality and diversity\n",
        "    - Correct label mapping\n",
        "    - Class balance\n",
        "    \"\"\"\n",
        "    shown_labels = set()\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(15, 8))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    sample_idx = 0\n",
        "    for i, sample in enumerate(dataset['train']):\n",
        "        label_id = sample['label']\n",
        "        label_name = id2label[label_id]\n",
        "        \n",
        "        if label_id not in shown_labels and sample_idx < num_classes:\n",
        "            axes[sample_idx].imshow(sample['image'])\n",
        "            axes[sample_idx].set_title(f\"{label_name} (ID: {label_id})\")\n",
        "            axes[sample_idx].axis('off')\n",
        "            shown_labels.add(label_id)\n",
        "            sample_idx += 1\n",
        "            \n",
        "        if len(shown_labels) == num_classes:\n",
        "            break\n",
        "    \n",
        "    # üîπ Hide unused subplots\n",
        "    for i in range(sample_idx, len(axes)):\n",
        "        axes[i].axis('off')\n",
        "        \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd3d0b95",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîπ Visualize dataset samples\n",
        "visualize_samples(dataset, id2label)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f130d33",
      "metadata": {},
      "source": [
        "## 5Ô∏è‚É£ **Vision Transformer Setup**\n",
        "\n",
        "### Initialize ViT Processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48cbdfac",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîπ Initialize the ViT processor\n",
        "# This handles image preprocessing according to the pre-trained model's requirements\n",
        "model_name = \"google/vit-large-patch16-224\"\n",
        "processor = ViTImageProcessor.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1f76462",
      "metadata": {},
      "source": [
        "### Processor Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b064525",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîπ Get processor parameters for data augmentation\n",
        "image_mean, image_std = processor.image_mean, processor.image_std\n",
        "size = processor.size[\"height\"]\n",
        "\n",
        "print(f\"Image size: {size}x{size}\")\n",
        "print(f\"Image mean: {image_mean}\")  \n",
        "print(f\"Image std: {image_std}\")    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8c0403f",
      "metadata": {},
      "source": [
        "## 6Ô∏è‚É£ **Data Augmentation & Preprocessing**\n",
        "\n",
        "### Define Transform Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea477a0c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîπ Define transformations\n",
        "# Important: Use the same normalization as the pre-trained model!\n",
        "normalize = Normalize(mean=image_mean, std=image_std)\n",
        "\n",
        "# üîπ Enhanced data augmentation for animals\n",
        "# These augmentations help the model generalize better\n",
        "train_transforms = Compose([\n",
        "    RandomResizedCrop(size, scale=(0.8, 1.0)),  # Random crop with scaling\n",
        "    RandomHorizontalFlip(p=0.5),                # 50% chance to flip horizontally\n",
        "    RandomRotation(degrees=15),                 # Small rotations\n",
        "    ColorJitter(brightness=0.2, contrast=0.2,  # Color variations\n",
        "               saturation=0.2, hue=0.1),\n",
        "    ToTensor(),                                 # Convert to tensor\n",
        "    normalize,                                  # ImageNet normalization\n",
        "])\n",
        "\n",
        "# üîπ Validation transforms (no augmentation, just preprocessing)\n",
        "val_transforms = Compose([\n",
        "    Resize(size),        # Resize to model input size\n",
        "    CenterCrop(size),    # Center crop to exact size\n",
        "    ToTensor(),          # Convert to tensor\n",
        "    normalize,           # ImageNet normalization\n",
        "])\n",
        "\n",
        "# üîπ Test transforms (same as validation)\n",
        "test_transforms = Compose([\n",
        "    Resize(size),\n",
        "    CenterCrop(size),\n",
        "    ToTensor(), \n",
        "    normalize,\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a6797e7",
      "metadata": {},
      "source": [
        "### Transform Application Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6639eddc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîπ Transform functions for different dataset splits\n",
        "def apply_train_transforms(examples):\n",
        "    \"\"\"Apply training transforms with data augmentation\"\"\"\n",
        "    examples['pixel_values'] = [train_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
        "    return examples\n",
        "\n",
        "def apply_val_transforms(examples):\n",
        "    \"\"\"Apply validation transforms without augmentation\"\"\"\n",
        "    examples['pixel_values'] = [val_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
        "    return examples\n",
        "\n",
        "def apply_test_transforms(examples):\n",
        "    \"\"\"Apply test transforms without augmentation\"\"\"\n",
        "    examples['pixel_values'] = [test_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
        "    return examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f5f8f4c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîπ Apply transforms to datasets\n",
        "# set_transform applies transforms on-the-fly during training\n",
        "dataset['train'].set_transform(apply_train_transforms)\n",
        "dataset['validation'].set_transform(apply_val_transforms)\n",
        "dataset['test'].set_transform(apply_test_transforms)\n",
        "print(\"Transforms applied to all dataset splits\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92ff0722",
      "metadata": {},
      "source": [
        "## 7Ô∏è‚É£ **Data Loading Setup**\n",
        "\n",
        "### Custom Collate Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a8241a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn(examples):\n",
        "    \"\"\"\n",
        "    üîπ Custom collate function for batching\n",
        "    \n",
        "    Converts list of examples into batched tensors:\n",
        "    - Stacks pixel_values into batch dimension\n",
        "    - Converts labels to tensor format\n",
        "    \"\"\"\n",
        "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db0424ca",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîπ Create data loaders with smaller batch sizes for memory efficiency\n",
        "# ViT-Large requires significant GPU memory, so we use smaller batches\n",
        "train_dl = DataLoader(dataset['train'], collate_fn=collate_fn, batch_size=32, shuffle=True)\n",
        "val_dl = DataLoader(dataset['validation'], collate_fn=collate_fn, batch_size=32)\n",
        "test_dl = DataLoader(dataset['test'], collate_fn=collate_fn, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b9ddb44",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîπ Test batch preparation to ensure everything works\n",
        "batch = next(iter(train_dl))\n",
        "for k, v in batch.items():\n",
        "    if isinstance(v, torch.Tensor):\n",
        "        print(f\"{k}: {v.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfaa96d3",
      "metadata": {},
      "source": [
        "## 8Ô∏è‚É£ **Model Initialization**\n",
        "\n",
        "### Load Pre-trained ViT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb6b1673",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîπ Initialize the model\n",
        "# Load pre-trained ViT and adapt it for our 10-class classification task\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    model_name,\n",
        "    id2label=id2label,                    # Class ID to label mapping\n",
        "    label2id=label2id,                    # Label to class ID mapping\n",
        "    ignore_mismatched_sizes=True,         # Ignore classifier head size mismatch\n",
        "    num_labels=len(id2label)             # Number of output classes\n",
        ")\n",
        "\n",
        "print(f\"Model initialized with {len(id2label)} classes\")\n",
        "print(f\"Model architecture: {model_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2c26bfb",
      "metadata": {},
      "source": [
        "## 9Ô∏è‚É£ **Training Configuration**\n",
        "\n",
        "### Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb360e7a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    üîπ Compute accuracy and other metrics for evaluation\n",
        "    \n",
        "    Args:\n",
        "        eval_pred: Tuple of (predictions, labels)\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary containing computed metrics\n",
        "    \"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)  # Get predicted class\n",
        "    \n",
        "    # üîπ Calculate various metrics\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc277125",
      "metadata": {},
      "source": [
        "### Training Arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "593ee18b",
      "metadata": {},
      "outputs": [],
      "source": [
        "#pip show transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6078e70",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîπ Training arguments optimized for Animals10 (memory-efficient)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./vit-animals10-finetuned\",     # Model save directory\n",
        "    save_total_limit=2,                         # Keep only 2 best checkpoints\n",
        "    save_strategy=\"epoch\",                      # Save after each epoch\n",
        "    eval_strategy=\"epoch\",                      # Evaluate after each epoch\n",
        "    learning_rate=5e-5,                         # Learning rate for fine-tuning\n",
        "    per_device_train_batch_size=8,              # Small batch size for memory\n",
        "    per_device_eval_batch_size=8,               # Small batch size for memory\n",
        "    num_train_epochs=2,                         # Number of training epochs\n",
        "    weight_decay=0.01,                          # L2 regularization\n",
        "    load_best_model_at_end=True,               # Load best model after training\n",
        "    metric_for_best_model=\"accuracy\",           # Metric to determine best model\n",
        "    logging_dir='./logs',                       # TensorBoard logs directory\n",
        "    logging_steps=25,                           # Log every 25 steps\n",
        "    remove_unused_columns=False,                # Keep all dataset columns\n",
        "    dataloader_num_workers=2,                   # Number of data loading workers\n",
        "    warmup_steps=50,                           # Learning rate warmup steps\n",
        "    gradient_accumulation_steps=2,              # Accumulate gradients over 2 steps\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cbc2bb9",
      "metadata": {},
      "source": [
        "### Initialize Trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32e761a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîπ Initialize trainer\n",
        "# The Trainer class handles the training loop, evaluation, and checkpointing\n",
        "trainer = Trainer(\n",
        "    model=model,                                # Our ViT model\n",
        "    args=training_args,                         # Training configuration\n",
        "    train_dataset=dataset['train'],             # Training data\n",
        "    eval_dataset=dataset['validation'],         # Validation data\n",
        "    data_collator=collate_fn,                   # Custom collate function\n",
        "    tokenizer=processor,                        # Image processor (for consistency)\n",
        "    compute_metrics=compute_metrics,            # Evaluation metrics function\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7357764",
      "metadata": {},
      "source": [
        "## üîü **Model Training**\n",
        "\n",
        "### Execute Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddbc190c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîπ Start training!\n",
        "# This will fine-tune the pre-trained ViT on our animal classification task\n",
        "train_result = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05a0c510",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£1Ô∏è‚É£ **Model Evaluation**\n",
        "(Yes I ran out of numbers to use)\n",
        "### Test Set Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9386f2c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîπ Evaluate on test set\n",
        "print(\"Evaluating on test set...\")\n",
        "test_results = trainer.predict(dataset['test'])\n",
        "print(\"Test Results:\")\n",
        "for metric, value in test_results.metrics.items():\n",
        "    print(f\"  {metric}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ce41708",
      "metadata": {},
      "source": [
        "### Confusion Matrix Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "490f01bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîπ Get predictions and true labels for detailed analysis\n",
        "y_true = test_results.label_ids\n",
        "y_pred = test_results.predictions.argmax(axis=1)\n",
        "\n",
        "# üîπ Create confusion matrix\n",
        "labels = [id2label[i] for i in range(len(id2label))]\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# üîπ Plot confusion matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "disp.plot(xticks_rotation=45, values_format='d')\n",
        "plt.title('Confusion Matrix - Animals10 Classification')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfa87363",
      "metadata": {},
      "source": [
        "### Classification Report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e82207fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîπ Print detailed classification report\n",
        "print(\"Detailed Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcab8f3b",
      "metadata": {},
      "source": [
        "### Per-Class Accuracy Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec59edbe",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîπ Calculate per-class accuracy for detailed performance analysis\n",
        "print(\"\\nPer-class Accuracy:\")\n",
        "for i, label in enumerate(labels):\n",
        "    class_mask = (y_true == i)\n",
        "    if np.sum(class_mask) > 0:\n",
        "        class_acc = np.sum((y_pred == i) & class_mask) / np.sum(class_mask)\n",
        "        print(f\"  {label}: {class_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1856ac53",
      "metadata": {},
      "source": [
        "### **üîπ Exercise: Hyperparameter Experimentation**\n",
        "Try modifying these parameters and observe the effects:\n",
        "- **Learning Rate**: Try `1e-5`, `1e-4`, `5e-5`\n",
        "- **Data Augmentation**: Add/remove augmentation techniques\n",
        "- **Model Size**: Try `google/vit-large-patch16-224` or `convnext`(Don't kill me Eltayeb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20a699da",
      "metadata": {},
      "source": [
        "### Contributed by: Ali Habibullah"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
