{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "06420bf8",
      "metadata": {
        "id": "06420bf8",
        "papermill": {
          "duration": 0.003277,
          "end_time": "2025-07-26T12:32:44.823412",
          "exception": false,
          "start_time": "2025-07-26T12:32:44.820135",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "![image.png](https://i.imgur.com/a3uAqnb.png)\n",
        "\n",
        "# üéõÔ∏è Tajweed Audio Classification with EfficientNet-B0\n",
        "## Deep Learning for Quranic Recitation Analysis\n",
        "\n",
        "This comprehensive notebook demonstrates the power of **Convolutional Neural Networks (CNNs)** for audio classification tasks using mel spectrograms. We'll explore how to process audio data, convert it to visual representations, and train a state-of-the-art vision model for audio classification.\n",
        "\n",
        "### **üìå What We'll Cover:**\n",
        "\n",
        "1. **Audio Processing Pipeline**: Converting raw audio to mel spectrograms for CNN input\n",
        "2. **EfficientNet-B0 Architecture**: Using a pretrained vision model for audio classification\n",
        "3. **Cross-Validation Training**: Robust model evaluation with 5-fold stratified CV\n",
        "4. **Overfitting Analysis**: Understanding model performance and generalization\n",
        "5. **Feature Engineering**: Log-mel spectrograms as \"images\" for CNN processing\n",
        "\n",
        "### **üéØ Our Dataset: Tajweed Rules Classification**\n",
        "We'll use a dataset that classifies different Tajweed rules in Quranic recitation. This is a specialized audio classification task where we need to identify pronunciation rules based on audio features.\n",
        "\n",
        "### **üîä Audio-to-Vision Transfer Learning**\n",
        "Our approach converts audio signals into visual spectrograms, allowing us to leverage powerful vision models like EfficientNet for audio classification tasks. This technique has proven highly effective across many audio domains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xbltzHtsrvJL",
      "metadata": {
        "id": "xbltzHtsrvJL"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "# run this if you are working on colab(disgusting ü§Æ)\n",
        "!pip install torchmetrics\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j07XKlwvrVxi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j07XKlwvrVxi",
        "outputId": "08f4fe3b-6ec5-42cc-9f0a-2589708a3670"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"mohammad2012191/tajweed-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "782ddd03",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-08-04T19:59:33.142808Z",
          "iopub.status.busy": "2025-08-04T19:59:33.142483Z",
          "iopub.status.idle": "2025-08-04T19:59:33.151248Z",
          "shell.execute_reply": "2025-08-04T19:59:33.150472Z",
          "shell.execute_reply.started": "2025-08-04T19:59:33.142786Z"
        },
        "id": "782ddd03",
        "outputId": "5a48513b-9b0e-4e3f-8261-d335cb11709f",
        "papermill": {
          "duration": 16.398901,
          "end_time": "2025-07-26T12:33:01.225175",
          "exception": false,
          "start_time": "2025-07-26T12:32:44.826274",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "## üéõÔ∏è Configuration & Imports\n",
        "\n",
        "import random, os, gc\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn, torch.nn.functional as F\n",
        "import torchaudio\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from torchmetrics.classification import F1Score\n",
        "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "# reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# audio parameters\n",
        "SAMPLE_RATE = 16_000       # Hz\n",
        "MAX_SEC     = 4.0          # seconds\n",
        "MAX_LEN     = int(SAMPLE_RATE * MAX_SEC)  # samples\n",
        "\n",
        "# training parameters\n",
        "BATCH   = 32\n",
        "EPOCHS  = 5\n",
        "LR      = 3e-4\n",
        "K_FOLDS = 5\n",
        "\n",
        "# I/O paths\n",
        "WORK_DIR  = Path(path)\n",
        "TRAIN_CSV = WORK_DIR/\"train.csv\"\n",
        "TRAIN_DIR = WORK_DIR/\"train\"\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", DEVICE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "051bbd48",
      "metadata": {
        "id": "051bbd48",
        "papermill": {
          "duration": 0.002563,
          "end_time": "2025-07-26T12:33:01.230607",
          "exception": false,
          "start_time": "2025-07-26T12:33:01.228044",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## üîç Exploratory Data Analysis (EDA)\n",
        "\n",
        "Before diving into model training, we need to understand our dataset structure and characteristics. This section will help us:\n",
        "\n",
        "### **üéµ Key Analysis Steps:**\n",
        "1. **Dataset Overview**: Understanding file structure, sample counts, and label distribution\n",
        "2. **Reciter Analysis**: Examining which reciters are included and their representation\n",
        "3. **Label Distribution**: Checking class balance for our 4 Tajweed rules\n",
        "4. **Data Quality Assessment**: Ensuring our training data is well-distributed\n",
        "\n",
        "### **üìä Why EDA Matters for Audio:**\n",
        "- **Class Imbalance**: Audio datasets often have uneven class distributions\n",
        "- **Speaker Diversity**: Different reciters may have varying styles and audio quality\n",
        "- **Feature Understanding**: Knowing our target classes helps in model interpretation\n",
        "\n",
        "### **üé® Visualization Strategy:**\n",
        "We'll create visualizations to understand:\n",
        "- Reciter distribution across the dataset\n",
        "- Tajweed rule frequency and balance\n",
        "- Overall dataset characteristics that might affect training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad63e575",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "execution": {
          "iopub.execute_input": "2025-08-04T20:00:15.661715Z",
          "iopub.status.busy": "2025-08-04T20:00:15.661400Z",
          "iopub.status.idle": "2025-08-04T20:00:15.964657Z",
          "shell.execute_reply": "2025-08-04T20:00:15.963903Z",
          "shell.execute_reply.started": "2025-08-04T20:00:15.661693Z"
        },
        "id": "ad63e575",
        "outputId": "fa27f792-509d-4b91-c418-b6e9075a3540",
        "papermill": {
          "duration": 0.554294,
          "end_time": "2025-07-26T12:33:01.787418",
          "exception": false,
          "start_time": "2025-07-26T12:33:01.233124",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# üîç Load and explore the Tajweed dataset\n",
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "\n",
        "# Create label encodings for model training\n",
        "# LabelEncoder converts string labels to integers (required for PyTorch)\n",
        "le = LabelEncoder().fit(train_df[\"label_name\"])\n",
        "train_df[\"y\"] = le.transform(train_df[\"label_name\"])\n",
        "\n",
        "print(f\"üìä Dataset Overview:\")\n",
        "print(f\"   Total samples: {len(train_df):,}\")\n",
        "print(f\"   Number of classes: {len(le.classes_)}\")\n",
        "print(f\"   Classes: {list(le.classes_)}\")\n",
        "print(f\"   Number of reciters: {train_df.sheikh_name.nunique()}\")\n",
        "\n",
        "# Visualize reciter distribution - important for understanding data bias\n",
        "fig, axes = plt.subplots(1,1, figsize=(12,4), sharey=True)\n",
        "reciter_counts = train_df.sheikh_name.value_counts()\n",
        "reciter_counts.plot.bar(ax=axes, title=\"Reciter Distribution in Training Set\",\n",
        "                       color='steelblue', alpha=0.7)\n",
        "axes.set_xlabel(\"Reciter Name\")\n",
        "axes.set_ylabel(\"Number of Samples\")\n",
        "axes.tick_params(axis='x', rotation=45)\n",
        "axes.grid(True, alpha=0.3)\n",
        "\n",
        "# Add count labels on bars for better readability\n",
        "for i, (reciter, count) in enumerate(reciter_counts.items()):\n",
        "    axes.text(i, count + 10, str(count), ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analyze label distribution - crucial for understanding class balance\n",
        "print(f\"\\nüè∑Ô∏è Tajweed Rule Distribution:\")\n",
        "label_counts = train_df.label_name.value_counts()\n",
        "for rule, count in label_counts.items():\n",
        "    percentage = (count / len(train_df)) * 100\n",
        "    print(f\"   {rule:<15}: {count:4d} samples ({percentage:5.1f}%)\")\n",
        "\n",
        "# Check for class imbalance\n",
        "imbalance_ratio = label_counts.max() / label_counts.min()\n",
        "print(f\"\\n‚öñÔ∏è Class Balance Analysis:\")\n",
        "print(f\"   Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
        "if imbalance_ratio > 3:\n",
        "    print(\"   ‚ö†Ô∏è Significant class imbalance detected - consider stratified sampling\")\n",
        "else:\n",
        "    print(\"   ‚úÖ Reasonable class balance\")\n",
        "\n",
        "# Label histogram with enhanced styling\n",
        "plt.figure(figsize=(10,6))\n",
        "bars = label_counts.plot.bar(title=\"Tajweed Rule Distribution in Training Set\",\n",
        "                            color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'],\n",
        "                            alpha=0.8, edgecolor='black', linewidth=1)\n",
        "plt.xlabel(\"Tajweed Rule\")\n",
        "plt.ylabel(\"Number of Samples\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Add value labels and percentages on bars\n",
        "for i, (rule, count) in enumerate(label_counts.items()):\n",
        "    percentage = (count / len(train_df)) * 100\n",
        "    plt.text(i, count + 20, f'{count}\\n({percentage:.1f}%)',\n",
        "             ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7326dd93",
      "metadata": {
        "id": "7326dd93",
        "papermill": {
          "duration": 0.003706,
          "end_time": "2025-07-26T12:33:01.795309",
          "exception": false,
          "start_time": "2025-07-26T12:33:01.791603",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## üé∂ Audio Processing Pipeline: From Sound to Spectrograms\n",
        "\n",
        "This is where the magic happens! We'll convert raw audio files into **mel spectrograms** - visual representations that capture the essential frequency information in audio signals.\n",
        "\n",
        "### **üîä Understanding Mel Spectrograms:**\n",
        "\n",
        "**What is a Mel Spectrogram?**\n",
        "- **Time-Frequency Representation**: Shows how frequency content changes over time\n",
        "- **Mel Scale**: Perceptually-motivated frequency scale that mimics human hearing\n",
        "- **Visual Format**: Converts 1D audio ‚Üí 2D image that CNNs can process\n",
        "\n",
        "### **üõ†Ô∏è Our Processing Pipeline:**\n",
        "\n",
        "1. **Audio Loading**: Load WAV files and resample to consistent 16kHz\n",
        "2. **Mono Conversion**: Convert stereo to mono for consistency\n",
        "3. **Mel Transform**: Apply mel spectrogram with 128 mel bins\n",
        "4. **Log Scaling**: Apply logarithmic scaling for better dynamic range\n",
        "5. **Normalization**: Standardize values for stable training\n",
        "6. **Padding/Trimming**: Fixed-length sequences for batch processing\n",
        "7. **Channel Replication**: Convert 1-channel spectrogram to 3-channel \"RGB\" image\n",
        "\n",
        "### **‚öôÔ∏è Key Parameters:**\n",
        "- **Sample Rate**: 16kHz (standard for speech/audio ML)\n",
        "- **Max Duration**: 4 seconds (captures typical Tajweed phrase length)\n",
        "- **Mel Bins**: 128 (good balance between detail and computational efficiency)\n",
        "- **FFT Settings**: 1024 FFT size, 512 hop length for good time-frequency resolution\n",
        "\n",
        "### **üéØ Why This Approach Works:**\n",
        "- **Transfer Learning**: Pretrained vision models can process spectrogram \"images\"\n",
        "- **Rich Representation**: Mel spectrograms capture both temporal and spectral patterns\n",
        "- **Proven Success**: This technique has achieved SOTA results in many audio tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "558c1e48",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-04T20:01:15.228727Z",
          "iopub.status.busy": "2025-08-04T20:01:15.227964Z",
          "iopub.status.idle": "2025-08-04T20:01:15.236310Z",
          "shell.execute_reply": "2025-08-04T20:01:15.235483Z",
          "shell.execute_reply.started": "2025-08-04T20:01:15.228700Z"
        },
        "id": "558c1e48",
        "papermill": {
          "duration": 0.012891,
          "end_time": "2025-07-26T12:33:01.826093",
          "exception": false,
          "start_time": "2025-07-26T12:33:01.813202",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class TajweedSpecDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset for Tajweed audio classification.\n",
        "\n",
        "    Converts audio files to log-mel spectrograms suitable for CNN training.\n",
        "    Each audio file is processed into a 3-channel 'image' that EfficientNet can handle.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, df, folder, return_id=False):\n",
        "        \"\"\"\n",
        "        Initialize the dataset.\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame with audio file IDs and labels\n",
        "            folder: Path to folder containing audio files\n",
        "            return_id: If True, return file ID instead of label (for inference)\n",
        "        \"\"\"\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.folder = Path(folder)\n",
        "        self.return_id = return_id\n",
        "\n",
        "        # Create mel spectrogram transform\n",
        "        # These parameters are carefully chosen for good time-frequency resolution\n",
        "        self.mel = torchaudio.transforms.MelSpectrogram(\n",
        "            sample_rate=SAMPLE_RATE,    # 16kHz - standard for speech\n",
        "            n_fft=1024,                 # FFT window size - good frequency resolution\n",
        "            hop_length=512,             # Step size - 50% overlap for smooth transitions\n",
        "            n_mels=128                  # Number of mel bins - captures human hearing range\n",
        "        )\n",
        "\n",
        "        print(f\"   üéµ Dataset initialized with {len(self.df)} samples\")\n",
        "        print(f\"   üìÅ Audio folder: {self.folder}\")\n",
        "        print(f\"   üîß Mel spectrogram: {SAMPLE_RATE}Hz, {1024}FFT, {128}mel bins\")\n",
        "\n",
        "    def _pad_trim(self, spec):\n",
        "        \"\"\"\n",
        "        Ensure all spectrograms have the same time dimension.\n",
        "\n",
        "        Fixed-length sequences are essential for batch training.\n",
        "        We pad short sequences and trim long ones to MAX_LEN.\n",
        "        \"\"\"\n",
        "        # Calculate target time dimension based on hop length\n",
        "        T = MAX_LEN // 512  # 512 is hop_length\n",
        "\n",
        "        if spec.size(-1) < T:\n",
        "            # Pad short sequences with zeros\n",
        "            return F.pad(spec, (0, T - spec.size(-1)))\n",
        "\n",
        "        # Trim long sequences\n",
        "        return spec[..., :T]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Load and process a single audio sample.\n",
        "\n",
        "        Returns:\n",
        "            img: 3-channel mel spectrogram \"image\" [3, 128, T]\n",
        "            target: Either label (int) or file ID (str)\n",
        "        \"\"\"\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        # Load audio file\n",
        "        path = self.folder / f\"{row['id']}.wav\"\n",
        "        try:\n",
        "            wav, sr = torchaudio.load(str(path))\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error loading {path}: {e}\")\n",
        "            # Return a zero tensor if file can't be loaded\n",
        "            wav = torch.zeros((1, MAX_LEN))\n",
        "            sr = SAMPLE_RATE\n",
        "\n",
        "        # Resample if necessary (ensure consistent sample rate)\n",
        "        if sr != SAMPLE_RATE:\n",
        "            wav = torchaudio.functional.resample(wav, sr, SAMPLE_RATE)\n",
        "\n",
        "        # Convert to mono by averaging channels\n",
        "        # Shape: [channels, time] ‚Üí [1, time]\n",
        "        wav = wav.mean(0, keepdim=True)\n",
        "\n",
        "        # Apply mel spectrogram transform\n",
        "        # Shape: [1, time] ‚Üí [1, mel_bins, time_frames]\n",
        "        spec = self.mel(wav)\n",
        "\n",
        "        # Apply log scaling for better dynamic range\n",
        "        # Log scaling compresses large values and expands small ones\n",
        "        spec = 10 * torch.log10(spec + 1e-6)  # Add epsilon to avoid log(0)\n",
        "\n",
        "        # Ensure fixed time dimension for batch processing\n",
        "        spec = self._pad_trim(spec)\n",
        "\n",
        "        # Normalize to zero mean, unit variance for stable training\n",
        "        # Each sample is independently normalized\n",
        "        spec = (spec - spec.mean()) / (spec.std() + 1e-6)\n",
        "\n",
        "        # Convert to 3-channel \"RGB\" image for EfficientNet\n",
        "        # EfficientNet expects 3-channel input, so we replicate the mel spectrogram\n",
        "        img = spec.repeat(3, 1, 1)  # [1, 128, T] ‚Üí [3, 128, T]\n",
        "\n",
        "        # Return appropriate target based on mode\n",
        "        if self.return_id:\n",
        "            return img, row[\"id\"]  # For inference/testing\n",
        "        else:\n",
        "            return img, torch.tensor(row[\"y\"], dtype=torch.long)  # For training\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the total number of samples in the dataset.\"\"\"\n",
        "        return len(self.df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "578ba354",
      "metadata": {
        "id": "578ba354",
        "papermill": {
          "duration": 0.003185,
          "end_time": "2025-07-26T12:33:01.832769",
          "exception": false,
          "start_time": "2025-07-26T12:33:01.829584",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## üîä Visualizing Audio as Images: Sample Spectrograms\n",
        "\n",
        "Now let's see what our audio data looks like when converted to visual spectrograms! This visualization helps us understand:\n",
        "\n",
        "### **üéØ What We're Looking For:**\n",
        "- **Frequency Patterns**: Different Tajweed rules should show distinct frequency signatures\n",
        "- **Temporal Structure**: How frequency content changes over time\n",
        "- **Visual Differences**: Whether classes are visually distinguishable\n",
        "- **Data Quality**: Checking for any obvious artifacts or issues\n",
        "\n",
        "### **üìä Interpretation Guide:**\n",
        "- **Vertical Axis**: Frequency (mel bins) - lower frequencies at bottom, higher at top\n",
        "- **Horizontal Axis**: Time - how the sound evolves from left to right  \n",
        "- **Color Intensity**: Magnitude - brighter areas indicate stronger frequency components\n",
        "- **Patterns**: Each Tajweed rule should have characteristic visual \"fingerprints\"\n",
        "\n",
        "### **üîç What to Notice:**\n",
        "- **Harmonic Structure**: Parallel horizontal lines indicate voiced sounds\n",
        "- **Noise Patterns**: Random patterns may indicate unvoiced sounds or background noise\n",
        "- **Temporal Dynamics**: How the spectrum changes throughout the pronunciation\n",
        "- **Class Differences**: Visual patterns that might distinguish between Tajweed rules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9d54322",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "execution": {
          "iopub.execute_input": "2025-08-04T20:01:19.537350Z",
          "iopub.status.busy": "2025-08-04T20:01:19.537080Z",
          "iopub.status.idle": "2025-08-04T20:01:20.174436Z",
          "shell.execute_reply": "2025-08-04T20:01:20.173588Z",
          "shell.execute_reply.started": "2025-08-04T20:01:19.537331Z"
        },
        "id": "d9d54322",
        "outputId": "8ba532ed-9646-47aa-b2f9-73f938782a5f",
        "papermill": {
          "duration": 0.62437,
          "end_time": "2025-07-26T12:33:02.460447",
          "exception": false,
          "start_time": "2025-07-26T12:33:01.836077",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# üé® Visualize sample mel spectrograms to understand our data\n",
        "print(\"üîç Generating sample spectrograms for visual inspection...\")\n",
        "\n",
        "# Select diverse samples - one from each class if possible\n",
        "sample_indices = []\n",
        "for class_name in train_df['label_name'].unique()[:4]:\n",
        "    class_samples = train_df[train_df['label_name'] == class_name]\n",
        "    if len(class_samples) > 0:\n",
        "        sample_indices.append(class_samples.sample(1, random_state=SEED).index[0])\n",
        "\n",
        "# If we don't have 4 classes, fill with random samples\n",
        "while len(sample_indices) < 4:\n",
        "    remaining = train_df.index.difference(sample_indices)\n",
        "    sample_indices.append(np.random.choice(remaining))\n",
        "\n",
        "sample = train_df.iloc[sample_indices]\n",
        "ds = TajweedSpecDataset(sample, TRAIN_DIR, return_id=False)\n",
        "\n",
        "print(f\"üìä Sample Details:\")\n",
        "for i, (idx, row) in enumerate(sample.iterrows()):\n",
        "    print(f\"   Sample {i+1}: {row['label_name']} (Reciter: {row['sheikh_name']})\")\n",
        "\n",
        "# Create enhanced visualization\n",
        "plt.figure(figsize=(16, 10))\n",
        "\n",
        "for i, (img, lbl) in enumerate(ds):\n",
        "    # Use the first channel of our 3-channel image\n",
        "    spectrogram = img[0].numpy()\n",
        "    sample_info = sample.iloc[i]\n",
        "\n",
        "    plt.subplot(2, 2, i+1)\n",
        "\n",
        "    # Create the spectrogram plot with custom colormap\n",
        "    im = plt.imshow(spectrogram, aspect='auto', origin='lower',\n",
        "                   cmap='magma', interpolation='bilinear')\n",
        "\n",
        "    # Enhanced title with multiple pieces of information\n",
        "    plt.title(f'Class: {sample_info[\"label_name\"]}\\n'\n",
        "             f'Reciter: {sample_info[\"sheikh_name\"]}\\n'\n",
        "             f'Shape: {spectrogram.shape}',\n",
        "             fontsize=11, fontweight='bold', pad=10)\n",
        "\n",
        "    # Add axis labels and formatting\n",
        "    plt.xlabel('Time Frames', fontsize=10)\n",
        "    plt.ylabel('Mel Frequency Bins', fontsize=10)\n",
        "\n",
        "    # Add colorbar for this subplot\n",
        "    cbar = plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "    cbar.set_label('Log Magnitude', rotation=270, labelpad=15, fontsize=9)\n",
        "\n",
        "    # Add grid for better readability\n",
        "    plt.grid(True, alpha=0.2, linestyle='--')\n",
        "\n",
        "    # Add some statistics as text\n",
        "    mean_val = spectrogram.mean()\n",
        "    std_val = spectrogram.std()\n",
        "    plt.text(0.02, 0.98, f'Œº={mean_val:.2f}\\nœÉ={std_val:.2f}',\n",
        "             transform=plt.gca().transAxes, verticalalignment='top',\n",
        "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
        "             fontsize=8)\n",
        "\n",
        "plt.suptitle('Mel Spectrogram Samples: Audio ‚Üí Visual Representation',\n",
        "             fontsize=16, fontweight='bold', y=0.95)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print processing statistics\n",
        "print(f\"\\nüìà Mel Spectrogram Statistics:\")\n",
        "print(f\"   Input audio length: {MAX_SEC} seconds @ {SAMPLE_RATE}Hz = {MAX_LEN:,} samples\")\n",
        "print(f\"   Spectrogram dimensions: [3 channels, 128 mel bins, {MAX_LEN//512} time frames]\")\n",
        "print(f\"   Total features per sample: {3 * 128 * (MAX_LEN//512):,}\")\n",
        "print(f\"   Memory per batch ({BATCH} samples): ~{(3 * 128 * (MAX_LEN//512) * BATCH * 4 / 1024**2):.1f} MB\")\n",
        "\n",
        "# Analyze visual differences between classes\n",
        "print(f\"\\nüîç Visual Pattern Analysis:\")\n",
        "print(f\"   Look for distinct patterns in frequency distributions\")\n",
        "print(f\"   Notice temporal evolution differences between classes\")\n",
        "print(f\"   Observe harmonic structure variations across Tajweed rules\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23a91e29",
      "metadata": {
        "id": "23a91e29",
        "papermill": {
          "duration": 0.006807,
          "end_time": "2025-07-26T12:33:02.475316",
          "exception": false,
          "start_time": "2025-07-26T12:33:02.468509",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## üèóÔ∏è Model Architecture: EfficientNet-B0 for Audio Classification\n",
        "\n",
        "Now we'll build our classification model using **EfficientNet-B0**, a state-of-the-art convolutional neural network originally designed for image classification. We're applying it to our mel spectrogram \"images\" - a powerful example of transfer learning across domains.\n",
        "\n",
        "### **üß† Why EfficientNet-B0?**\n",
        "\n",
        "**EfficientNet Advantages:**\n",
        "- **Efficiency**: Optimized architecture balancing accuracy, speed, and model size\n",
        "- **Compound Scaling**: Systematically scales depth, width, and resolution together\n",
        "- **Transfer Learning**: Pretrained on ImageNet, providing rich feature representations\n",
        "- **Proven Performance**: SOTA results across many vision tasks, transfers well to audio\n",
        "\n",
        "### **üîÑ Architecture Adaptation:**\n",
        "\n",
        "**Original Design**: Image classification (224√ó224 RGB ‚Üí 1000 classes)\n",
        "**Our Adaptation**: Audio spectrograms (128√óT mel-spectrogram ‚Üí 4 Tajweed classes)\n",
        "\n",
        "### **üéØ Key Modifications:**\n",
        "1. **Input Compatibility**: Our mel spectrograms are already 3-channel \"RGB-like\" images\n",
        "2. **Output Layer**: Replace final classifier for our 4 Tajweed classes\n",
        "3. **Pretrained Weights**: Start with ImageNet weights for faster convergence\n",
        "4. **Fine-tuning**: All layers trainable to adapt features for audio domain\n",
        "\n",
        "### **‚ö° Model Benefits:**\n",
        "- **Fast Training**: Pretrained features reduce training time\n",
        "- **Good Generalization**: ImageNet features often transfer well to other domains\n",
        "- **Memory Efficient**: EfficientNet-B0 is the smallest variant, perfect for our task size\n",
        "- **Robust Features**: Deep CNN learns hierarchical audio-visual patterns\n",
        "\n",
        "### **üîß Training Strategy:**\n",
        "We'll use the pretrained model as a feature extractor that learns to recognize audio patterns through the visual spectrogram representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "453655f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-08-04T20:01:24.033751Z",
          "iopub.status.busy": "2025-08-04T20:01:24.033034Z",
          "iopub.status.idle": "2025-08-04T20:01:24.037389Z",
          "shell.execute_reply": "2025-08-04T20:01:24.036613Z",
          "shell.execute_reply.started": "2025-08-04T20:01:24.033721Z"
        },
        "id": "453655f6",
        "outputId": "c5ff2cdd-309d-46dc-f53b-a6b154c193fb",
        "papermill": {
          "duration": 0.014286,
          "end_time": "2025-07-26T12:33:02.497150",
          "exception": false,
          "start_time": "2025-07-26T12:33:02.482864",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def build_model(n_classes):\n",
        "    \"\"\"\n",
        "    Build EfficientNet-B0 model adapted for Tajweed audio classification.\n",
        "\n",
        "    Args:\n",
        "        n_classes: Number of output classes (4 for our Tajweed rules)\n",
        "\n",
        "    Returns:\n",
        "        model: PyTorch model ready for training\n",
        "    \"\"\"\n",
        "    print(f\"üèóÔ∏è Building EfficientNet-B0 for {n_classes} classes...\")\n",
        "\n",
        "    # Load pretrained EfficientNet-B0 with ImageNet weights\n",
        "    # This gives us powerful feature representations learned from millions of images\n",
        "    model = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
        "\n",
        "    # Print original architecture info\n",
        "    original_features = model.classifier[1].in_features\n",
        "    print(f\"   üìä Original classifier input features: {original_features}\")\n",
        "    print(f\"   üîÑ Original output classes: 1000 (ImageNet)\")\n",
        "\n",
        "    # Replace the final classifier layer for our specific task\n",
        "    # We keep all the convolutional feature extraction layers unchanged\n",
        "    model.classifier = nn.Linear(original_features, n_classes)\n",
        "\n",
        "    print(f\"   ‚úÖ New classifier: {original_features} ‚Üí {n_classes} classes\")\n",
        "    print(f\"   üéØ Target classes: {n_classes} Tajweed rules\")\n",
        "\n",
        "    # Move model to appropriate device (GPU if available)\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    # Print model summary\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"   üìà Total parameters: {total_params:,}\")\n",
        "    print(f\"   üîß Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"   üíæ Estimated model size: {total_params * 4 / 1024**2:.1f} MB\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Test model creation\n",
        "print(\"üß™ Testing model creation...\")\n",
        "test_model = build_model(n_classes=4)\n",
        "\n",
        "# Test forward pass with dummy input to verify dimensions\n",
        "print(f\"\\nüîß Testing model input/output dimensions...\")\n",
        "dummy_input = torch.randn(2, 3, 128, MAX_LEN//512).to(DEVICE)  # Batch of 2 samples\n",
        "print(f\"   Input shape: {dummy_input.shape}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    dummy_output = test_model(dummy_input)\n",
        "    print(f\"   Output shape: {dummy_output.shape}\")\n",
        "    print(f\"   ‚úÖ Model accepts our mel spectrogram format!\")\n",
        "\n",
        "# Clean up test model\n",
        "del test_model, dummy_input, dummy_output\n",
        "torch.cuda.empty_cache() if torch.cuda.is_available() else None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4768b7f7",
      "metadata": {
        "id": "4768b7f7",
        "papermill": {
          "duration": 0.007091,
          "end_time": "2025-07-26T12:33:02.511296",
          "exception": false,
          "start_time": "2025-07-26T12:33:02.504205",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## üîÑ Cross-Validation Training: Robust Model Evaluation\n",
        "\n",
        "We'll implement **5-fold stratified cross-validation** to ensure our model generalizes well. This approach gives us a more reliable estimate of model performance than a single train-test split.\n",
        "\n",
        "### **üéØ Why 5-Fold Stratified CV?**\n",
        "\n",
        "**Stratified Sampling Benefits:**\n",
        "- **Balanced Folds**: Each fold maintains the same class distribution as the original dataset\n",
        "- **Reduced Variance**: Multiple validation scores provide more robust performance estimates  \n",
        "- **Better Generalization**: Tests model performance across different data subsets\n",
        "- **Class Balance**: Ensures all Tajweed rules are represented in each fold\n",
        "\n",
        "### **üìä Our Training Protocol:**\n",
        "\n",
        "**For Each Fold (1-5):**\n",
        "1. **Data Split**: 80% training, 20% validation (stratified by class)\n",
        "2. **Model Initialization**: Fresh EfficientNet-B0 with pretrained weights\n",
        "3. **Training Loop**: Multiple epochs with Adam optimizer + Cosine LR scheduling\n",
        "4. **Validation**: Track accuracy and F1-score after each epoch\n",
        "5. **Best Model Selection**: Save checkpoint with highest validation F1\n",
        "6. **Performance Tracking**: Record training curves and metrics\n",
        "\n",
        "### **‚öôÔ∏è Training Configuration:**\n",
        "- **Optimizer**: AdamW (Adam with weight decay for better generalization)\n",
        "- **Learning Rate**: 3e-4 (conservative for fine-tuning pretrained model)\n",
        "- **Scheduler**: Cosine Annealing (smooth LR decay over epochs)\n",
        "- **Loss Function**: CrossEntropyLoss (standard for multi-class classification)\n",
        "- **Metric**: Macro F1-Score (handles class imbalance better than accuracy)\n",
        "- **Batch Size**: 32 (good balance of stability and memory usage)\n",
        "\n",
        "### **üìà What We'll Monitor:**\n",
        "- **Training Loss**: Should decrease steadily\n",
        "- **Validation Loss**: Should decrease without diverging from training loss\n",
        "- **Validation F1**: Our primary metric for model selection\n",
        "- **Overfitting**: Gap between training and validation performance\n",
        "- **Convergence**: How quickly the model learns across folds\n",
        "\n",
        "### **üé® Visualization Strategy:**\n",
        "We'll plot learning curves for each fold to understand:\n",
        "- Training dynamics and convergence speed\n",
        "- Overfitting patterns and when to stop training\n",
        "- Consistency across different data splits\n",
        "- Optimal number of epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23dcbc60-7a5f-4d72-b67e-402dd9ebbdb5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-04T20:02:42.412097Z",
          "iopub.status.busy": "2025-08-04T20:02:42.411425Z",
          "iopub.status.idle": "2025-08-04T20:02:42.418570Z",
          "shell.execute_reply": "2025-08-04T20:02:42.417814Z",
          "shell.execute_reply.started": "2025-08-04T20:02:42.412066Z"
        },
        "id": "23dcbc60-7a5f-4d72-b67e-402dd9ebbdb5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def plot_loss(train_losses, val_losses, fold=None):\n",
        "    \"\"\"\n",
        "    Plot training and validation loss curves for a single fold.\n",
        "\n",
        "    Args:\n",
        "        train_losses: List of training losses per epoch\n",
        "        val_losses: List of validation losses per epoch\n",
        "        fold: Fold number for title (optional)\n",
        "    \"\"\"\n",
        "    epochs = list(range(1, len(train_losses) + 1))\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Plot both curves with distinct styling\n",
        "    train_line = plt.plot(epochs, train_losses, 'o-', label='Training Loss',\n",
        "                         linewidth=2, markersize=4, color='#3498db', alpha=0.8)\n",
        "    val_line = plt.plot(epochs, val_losses, 's-', label='Validation Loss',\n",
        "                       linewidth=2, markersize=4, color='#e74c3c', alpha=0.8)\n",
        "\n",
        "    # Formatting and annotations\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Cross-Entropy Loss', fontsize=12)\n",
        "\n",
        "    title = f'Fold {fold} - Loss Curves' if fold is not None else 'Loss Curves'\n",
        "    plt.title(title, fontsize=14, fontweight='bold')\n",
        "\n",
        "    plt.legend(fontsize=11)\n",
        "    plt.grid(True, alpha=0.3, linestyle='--')\n",
        "\n",
        "    # Add minimum validation loss annotation\n",
        "    min_val_epoch = np.argmin(val_losses) + 1\n",
        "    min_val_loss = min(val_losses)\n",
        "    plt.annotate(f'Min Val Loss: {min_val_loss:.4f}\\nEpoch {min_val_epoch}',\n",
        "                xy=(min_val_epoch, min_val_loss),\n",
        "                xytext=(min_val_epoch + len(epochs)*0.1, min_val_loss + max(val_losses)*0.05),\n",
        "                arrowprops=dict(arrowstyle='->', color='red', alpha=0.7),\n",
        "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7),\n",
        "                fontsize=10)\n",
        "\n",
        "    # Add convergence analysis\n",
        "    if len(val_losses) >= 3:\n",
        "        recent_improvement = val_losses[-3] - val_losses[-1]\n",
        "        if recent_improvement < 0.001:\n",
        "            plt.text(0.02, 0.98, f'‚ö†Ô∏è Converged\\n(Œî={recent_improvement:.4f})',\n",
        "                    transform=plt.gca().transAxes, verticalalignment='top',\n",
        "                    bbox=dict(boxstyle='round', facecolor='orange', alpha=0.7),\n",
        "                    fontsize=9)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_score(val_f1s, fold=None):\n",
        "    \"\"\"\n",
        "    Plot validation F1-score progression for a single fold.\n",
        "\n",
        "    Args:\n",
        "        val_f1s: List of validation F1-scores per epoch\n",
        "        fold: Fold number for title (optional)\n",
        "    \"\"\"\n",
        "    epochs = list(range(1, len(val_f1s) + 1))\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Plot F1 progression with markers for clarity\n",
        "    f1_line = plt.plot(epochs, val_f1s, 'o-', label='Validation F1-Score',\n",
        "                      linewidth=3, markersize=6, color='#2ecc71',\n",
        "                      markerfacecolor='lightgreen', markeredgecolor='darkgreen')\n",
        "\n",
        "    # Formatting\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Macro F1-Score', fontsize=12)\n",
        "\n",
        "    title = f'Fold {fold} - F1-Score Progression' if fold is not None else 'F1-Score Progression'\n",
        "    plt.title(title, fontsize=14, fontweight='bold')\n",
        "\n",
        "    plt.legend(fontsize=11)\n",
        "    plt.grid(True, alpha=0.3, linestyle='--')\n",
        "\n",
        "    # Set y-axis limits for better visualization\n",
        "    plt.ylim(max(0, min(val_f1s) - 0.05), min(1.0, max(val_f1s) + 0.05))\n",
        "\n",
        "    # Add best F1 annotation\n",
        "    best_epoch = np.argmax(val_f1s) + 1\n",
        "    best_f1 = max(val_f1s)\n",
        "    plt.annotate(f'Best F1: {best_f1:.4f}\\nEpoch {best_epoch}',\n",
        "                xy=(best_epoch, best_f1),\n",
        "                xytext=(best_epoch + len(epochs)*0.1, best_f1 - 0.03),\n",
        "                arrowprops=dict(arrowstyle='->', color='green', alpha=0.7),\n",
        "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\", alpha=0.8),\n",
        "                fontsize=10, fontweight='bold')\n",
        "\n",
        "    # Add performance indicators\n",
        "    if best_f1 > 0.8:\n",
        "        performance = \"Excellent\"\n",
        "    elif best_f1 > 0.7:\n",
        "        performance = \"Good\"\n",
        "    elif best_f1 > 0.6:\n",
        "        performance = \"Fair\"\n",
        "    else:\n",
        "        performance = \"Poor\"\n",
        "\n",
        "    plt.text(0.02, 0.02, f'Performance: {performance}',\n",
        "            transform=plt.gca().transAxes, verticalalignment='bottom',\n",
        "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
        "            fontsize=10, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33eb25a0",
      "metadata": {
        "id": "33eb25a0"
      },
      "source": [
        "## üöÄ Training Loop: 5-Fold Cross-Validation Execution\n",
        "\n",
        "Now for the main event! We'll train our EfficientNet-B0 model using 5-fold stratified cross-validation. This comprehensive training process will help us understand:\n",
        "\n",
        "### **üîÑ Training Process:**\n",
        "\n",
        "**Per Fold Execution:**\n",
        "1. **Data Preparation**: Create stratified train/validation splits\n",
        "2. **Model Setup**: Initialize fresh model, optimizer, scheduler, and metrics\n",
        "3. **Training Epochs**: Train for specified epochs with progress tracking\n",
        "4. **Validation**: Evaluate performance after each epoch\n",
        "5. **Best Model Saving**: Keep checkpoint with highest validation F1-score\n",
        "6. **Visualization**: Generate learning curves for analysis\n",
        "\n",
        "### **üìä Metrics We Track:**\n",
        "\n",
        "**Training Metrics:**\n",
        "- **Training Loss**: CrossEntropyLoss on training data\n",
        "- **Epoch Time**: Training speed monitoring\n",
        "- **Learning Rate**: Scheduler progression\n",
        "\n",
        "**Validation Metrics:**\n",
        "- **Validation Loss**: CrossEntropyLoss on validation data  \n",
        "- **Macro F1-Score**: Primary metric for model selection (handles class imbalance)\n",
        "- **Overfitting Detection**: Training vs validation performance gap\n",
        "\n",
        "### **üéõÔ∏è Hyperparameter Rationale:**\n",
        "\n",
        "**Learning Rate (3e-4)**: Conservative rate for fine-tuning pretrained model\n",
        "**Batch Size (32)**: Good balance of stability and memory efficiency\n",
        "**Epochs (5)**: Quick training for demonstration - typically would use more\n",
        "**Cosine Annealing**: Smooth learning rate decay for stable convergence\n",
        "\n",
        "### **üîç What to Expect:**\n",
        "\n",
        "**Typical Training Patterns:**\n",
        "- **Fast Initial Learning**: Pretrained features adapt quickly to audio spectrograms\n",
        "- **Gradual Convergence**: F1-scores should steadily improve across epochs\n",
        "- **Fold Variation**: Some folds may perform better due to data distribution\n",
        "- **Minimal Overfitting**: Good generalization expected with pretrained features\n",
        "\n",
        "**Success Indicators:**\n",
        "- Validation F1 > 0.7 (good performance for 4-class audio classification)\n",
        "- Consistent performance across folds\n",
        "- Stable training curves without excessive fluctuation\n",
        "- Reasonable training times (~1-2 minutes per fold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uhr6PjHJxQIF",
      "metadata": {
        "id": "uhr6PjHJxQIF"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e798f4ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "execution": {
          "iopub.execute_input": "2025-08-04T20:21:05.244283Z",
          "iopub.status.busy": "2025-08-04T20:21:05.243618Z",
          "iopub.status.idle": "2025-08-04T20:27:53.135129Z",
          "shell.execute_reply": "2025-08-04T20:27:53.134380Z",
          "shell.execute_reply.started": "2025-08-04T20:21:05.244252Z"
        },
        "id": "e798f4ef",
        "outputId": "17f38184-119c-440f-a684-9cef4a85e33b",
        "papermill": {
          "duration": 412.468141,
          "end_time": "2025-07-26T12:39:54.986357",
          "exception": false,
          "start_time": "2025-07-26T12:33:02.518216",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Initialize tracking variables\n",
        "f1_scores = []  # Store best F1 from each fold\n",
        "fold_results = {}  # Detailed results for each fold\n",
        "overall_start_time = time.time()\n",
        "\n",
        "# Create stratified k-fold splitter\n",
        "skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(skf.split(train_df, y=train_df.y), 1):\n",
        "    fold_start_time = time.time()\n",
        "\n",
        "    # Create data splits with detailed logging\n",
        "    train_split = train_df.iloc[tr_idx]\n",
        "    val_split = train_df.iloc[va_idx]\n",
        "\n",
        "\n",
        "    # Check class distribution in splits\n",
        "    train_dist = train_split['label_name'].value_counts()\n",
        "    val_dist = val_split['label_name'].value_counts()\n",
        "\n",
        "\n",
        "    # Create data loaders with appropriate settings\n",
        "    tr_dl = DataLoader(\n",
        "        TajweedSpecDataset(train_split, TRAIN_DIR),\n",
        "        batch_size=BATCH,\n",
        "        shuffle=True,      # Shuffle training data for better learning\n",
        "        num_workers=2,     # Parallel data loading\n",
        "        pin_memory=True,   # Faster GPU transfer\n",
        "        drop_last=True     # Drop incomplete batches for consistent training\n",
        "    )\n",
        "\n",
        "    va_dl = DataLoader(\n",
        "        TajweedSpecDataset(val_split, TRAIN_DIR),\n",
        "        batch_size=BATCH,\n",
        "        shuffle=False,     # No shuffle needed for validation\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Initialize model and training components\n",
        "    model = build_model(n_classes=4).to(DEVICE)\n",
        "\n",
        "    # AdamW optimizer with weight decay for regularization\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
        "\n",
        "    # Cosine annealing learning rate scheduler\n",
        "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n",
        "\n",
        "    # Loss function for multi-class classification\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "\n",
        "    # F1 score metric for evaluation (macro average handles class imbalance)\n",
        "    f1m = F1Score(task=\"multiclass\", num_classes=4, average=\"macro\").to(DEVICE)\n",
        "\n",
        "    # Training tracking variables\n",
        "    best_f1 = 0\n",
        "    train_losses, val_losses, val_f1s = [], [], []\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "\n",
        "    # Training loop for this fold\n",
        "    for ep in range(1, EPOCHS + 1):\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        # ============ TRAINING PHASE ============\n",
        "        model.train()  # Set model to training mode\n",
        "        running_loss = 0.0\n",
        "        samples_processed = 0\n",
        "\n",
        "        # Progress bar for training batches\n",
        "        train_pbar = tqdm(tr_dl, desc=f'Fold{fold} Ep{ep:02d} Train',\n",
        "                         leave=False, ncols=100)\n",
        "\n",
        "        for batch_idx, (xb, yb) in enumerate(train_pbar):\n",
        "            # Move data to device\n",
        "            xb, yb = xb.to(DEVICE, non_blocking=True), yb.to(DEVICE, non_blocking=True)\n",
        "\n",
        "            # Forward pass\n",
        "            opt.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = crit(logits, yb)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            # Track metrics\n",
        "            batch_loss = loss.item()\n",
        "            batch_size = xb.size(0)\n",
        "            running_loss += batch_loss * batch_size\n",
        "            samples_processed += batch_size\n",
        "\n",
        "            # Update progress bar\n",
        "            train_pbar.set_postfix({\n",
        "                'loss': f'{batch_loss:.4f}',\n",
        "                'avg_loss': f'{running_loss/samples_processed:.4f}'\n",
        "            })\n",
        "\n",
        "        # Calculate average training loss\n",
        "        epoch_train_loss = running_loss / samples_processed\n",
        "        train_losses.append(epoch_train_loss)\n",
        "\n",
        "        # ============ VALIDATION PHASE ============\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "        f1m.reset()   # Reset F1 metric\n",
        "        val_loss_acc = 0.0\n",
        "        val_samples = 0\n",
        "\n",
        "        # Progress bar for validation batches\n",
        "        val_pbar = tqdm(va_dl, desc=f'Fold{fold} Ep{ep:02d} Val',\n",
        "                       leave=False, ncols=100)\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient computation for efficiency\n",
        "            for batch_idx, (xb, yb) in enumerate(val_pbar):\n",
        "                xb, yb = xb.to(DEVICE, non_blocking=True), yb.to(DEVICE, non_blocking=True)\n",
        "\n",
        "                # Forward pass\n",
        "                logits = model(xb)\n",
        "                val_loss = crit(logits, yb)\n",
        "\n",
        "                # Accumulate validation loss\n",
        "                batch_size = xb.size(0)\n",
        "                val_loss_acc += val_loss.item() * batch_size\n",
        "                val_samples += batch_size\n",
        "\n",
        "                # Update F1 metric\n",
        "                f1m.update(logits.softmax(1), yb)\n",
        "\n",
        "                # Update progress bar\n",
        "                val_pbar.set_postfix({\n",
        "                    'val_loss': f'{val_loss.item():.4f}',\n",
        "                    'avg_val_loss': f'{val_loss_acc/val_samples:.4f}'\n",
        "                })\n",
        "\n",
        "        # Calculate validation metrics\n",
        "        epoch_val_loss = val_loss_acc / val_samples\n",
        "        val_losses.append(epoch_val_loss)\n",
        "\n",
        "        val_f1 = f1m.compute().item()\n",
        "        val_f1s.append(val_f1)\n",
        "\n",
        "        # Update learning rate\n",
        "        sched.step()\n",
        "        current_lr = opt.param_groups[0]['lr']\n",
        "\n",
        "        # Calculate epoch timing\n",
        "        epoch_time = time.time() - epoch_start_time\n",
        "\n",
        "        # Model checkpointing - save if this is the best validation F1\n",
        "        if val_f1 > best_f1:\n",
        "            best_f1 = val_f1\n",
        "            torch.save(model.state_dict(), f\"effb0_fold{fold}.pt\")\n",
        "            epochs_without_improvement = 0\n",
        "            improvement_indicator = \"üåü NEW BEST!\"\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            improvement_indicator = f\"({epochs_without_improvement} epochs since improvement)\"\n",
        "\n",
        "\n",
        "    # End of fold - generate visualizations and summary\n",
        "    fold_time = time.time() - fold_start_time\n",
        "\n",
        "\n",
        "\n",
        "    # Store fold results\n",
        "    fold_results[fold] = {\n",
        "        'best_f1': best_f1,\n",
        "        'final_train_loss': train_losses[-1],\n",
        "        'final_val_loss': val_losses[-1],\n",
        "        'training_time': fold_time,\n",
        "        'train_losses': train_losses.copy(),\n",
        "        'val_losses': val_losses.copy(),\n",
        "        'val_f1s': val_f1s.copy()\n",
        "    }\n",
        "\n",
        "    # Generate and display learning curves for this fold\n",
        "    plot_loss(train_losses, val_losses, fold)\n",
        "    plot_score(val_f1s, fold)\n",
        "\n",
        "    # Add best F1 to overall results\n",
        "    f1_scores.append(best_f1)\n",
        "\n",
        "\n",
        "    # Memory cleanup\n",
        "    del model, opt, sched, tr_dl, va_dl\n",
        "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vvGdaoZsxLY5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvGdaoZsxLY5",
        "outputId": "3f44185a-0324-42aa-ddf2-c7f4a43e73d7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============ OVERALL RESULTS SUMMARY ============\n",
        "total_time = time.time() - overall_start_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üèÅ CROSS-VALIDATION COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nüìä Overall Results Summary:\")\n",
        "print(f\"   ‚è±Ô∏è  Total training time: {total_time:.1f}s ({total_time/60:.1f} minutes)\")\n",
        "print(f\"   üéØ CV F1 scores: {[f'{score:.4f}' for score in f1_scores]}\")\n",
        "print(f\"   üìà Mean F1: {np.mean(f1_scores):.4f} ¬± {np.std(f1_scores):.4f}\")\n",
        "print(f\"   üèÜ Best fold F1: {max(f1_scores):.4f} (Fold {f1_scores.index(max(f1_scores))+1})\")\n",
        "print(f\"   üìâ Worst fold F1: {min(f1_scores):.4f} (Fold {f1_scores.index(min(f1_scores))+1})\")\n",
        "print(f\"   üìä F1 range: {max(f1_scores) - min(f1_scores):.4f}\")\n",
        "\n",
        "# Performance assessment\n",
        "mean_f1 = np.mean(f1_scores)\n",
        "if mean_f1 > 0.8:\n",
        "    assessment = \"üèÜ Excellent performance!\"\n",
        "elif mean_f1 > 0.7:\n",
        "    assessment = \"‚úÖ Good performance!\"\n",
        "elif mean_f1 > 0.6:\n",
        "    assessment = \"‚ö†Ô∏è Fair performance - consider hyperparameter tuning\"\n",
        "else:\n",
        "    assessment = \"üî¥ Poor performance - review data and model architecture\"\n",
        "\n",
        "print(f\"\\nüéØ Performance Assessment: {assessment}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Training complete! Model checkpoints saved for all folds.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jInM5sl_2y_b",
      "metadata": {
        "id": "jInM5sl_2y_b"
      },
      "source": [
        "## **üéØ Key Takeaways and Next Steps**\n",
        "\n",
        "### **üèÜ What We Achieved:**\n",
        "1. **Successfully applied transfer learning** from vision to audio domain\n",
        "2. **Robust cross-validation** with 5-fold stratified splits\n",
        "3. **Mel spectrogram pipeline** converting audio ‚Üí visual representations\n",
        "4. **EfficientNet-B0 adaptation** for Tajweed classification\n",
        "5. **Comprehensive training monitoring** with learning curves and metrics\n",
        "\n",
        "### **‚ùå Current Limitations:**\n",
        "1. **Limited epochs (5)** ‚Üí Models may not have fully converged\n",
        "2. **Small dataset** ‚Üí Limited generalization to diverse reciters\n",
        "3. **Fixed audio length (4s)** ‚Üí May truncate longer recitations\n",
        "4. **No data augmentation** ‚Üí Missing robustness to audio variations\n",
        "5. **Single model architecture** ‚Üí No ensemble methods tested\n",
        "6. **No confusion matrix analysis** ‚Üí Unclear which classes are confused\n",
        "\n",
        "\n",
        "### **üîπ Audio-Specific Challenges:**\n",
        "- **Background noise**: Real-world recordings have varying audio quality\n",
        "- **Speaker variability**: Different reciters have unique vocal characteristics\n",
        "   - ŸÖÿ≠ŸÖÿØ ÿµÿØŸäŸÇ ÿßŸÑŸÖŸÜÿ¥ÿßŸàŸä vs ÿπÿ®ÿØ ÿßŸÑÿ®ÿßÿ≥ÿ∑ ÿπÿ®ÿØ ÿßŸÑÿµŸÖÿØ vs ŸÖÿßŸáÿ± ÿßŸÑŸÖÿπŸäŸÇŸÑŸä\n",
        "- **Recording conditions**: Studio vs mosque vs outdoor environments\n",
        "- **Pronunciation variations**: Regional Arabic dialects affecting Tajweed\n",
        "- **Overlapping rules**: Multiple Tajweed concepts in single audio segment\n",
        "\n",
        "\n",
        "### **üì± Real-World Deployment Considerations:**\n",
        "- **Mobile optimization**: Model quantization and pruning for smartphones\n",
        "- **Latency requirements**: Real-time feedback for educational apps\n",
        "- **Cultural sensitivity**: Accurate representation of Islamic scholarship\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CU5_vVMt3LiR",
      "metadata": {
        "id": "CU5_vVMt3LiR"
      },
      "source": [
        "# Contributed by: Mohamed Eltayeb (He did everything) - Edited by: Ali Habibullah (I added comments and markdowns üò∂‚Äçüå´Ô∏è)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 8008858,
          "sourceId": 12673249,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31090,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 479.459827,
      "end_time": "2025-07-26T12:40:40.173758",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-07-26T12:32:40.713931",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
