{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "96835850",
      "metadata": {
        "id": "96835850"
      },
      "source": [
        "![image.png](https://i.imgur.com/a3uAqnb.png)\n",
        "\n",
        "# üéõÔ∏è 1D CNN for Tajweed Audio Classification\n",
        "\n",
        "In this notebook, we will implement a **1D Convolutional Neural Network (CNN)** for classifying **Tajweed rules** in Quranic recitation audio.\n",
        "\n",
        "**1D CNN** is a powerful architecture for processing sequential data like audio waveforms. Unlike traditional 2D CNNs used for images, 1D CNNs operate directly on the raw audio signal, learning temporal patterns and features that distinguish different Tajweed rules.\n",
        "\n",
        "This notebook demonstrates end-to-end audio classification using PyTorch and torchaudio.\n",
        "\n",
        "## üìå **What is 1D CNN for Audio?**\n",
        "\n",
        "**1D CNN** processes audio data by applying convolutions along the time dimension, making it ideal for:\n",
        "- Raw audio waveform analysis\n",
        "- Temporal pattern recognition\n",
        "- Feature extraction from sequential data\n",
        "- Real-time audio processing\n",
        "\n",
        "### üîπ **Key Concepts:**\n",
        "1Ô∏è‚É£ **Raw Waveform Processing**: Direct analysis of audio samples without spectrograms\n",
        "\n",
        "2Ô∏è‚É£ **Temporal Convolutions**: 1D kernels slide across time dimension\n",
        "\n",
        "3Ô∏è‚É£ **Hierarchical Features**: Early layers capture local patterns, deeper layers learn complex temporal structures\n",
        "\n",
        "4Ô∏è‚É£ **End-to-End Learning**: No manual feature engineering required\n",
        "\n",
        "### üîπ **1D CNN vs Traditional Audio Processing:**\n",
        "| **1D CNN** | **Traditional Methods** |\n",
        "|------------|-------------------------|\n",
        "| Raw waveform input | Hand-crafted features (MFCC, spectrograms) |\n",
        "| Automatic feature learning | Manual feature engineering |\n",
        "| End-to-end optimization | Multi-stage pipeline |\n",
        "| Temporal pattern recognition | Frequency-domain analysis |\n",
        "| GPU-accelerated | Often CPU-based |\n",
        "\n",
        "## üéØ **Tajweed Classification Task**\n",
        "\n",
        "**Tajweed** refers to the rules governing pronunciation during Quranic recitation. Our task is to classify audio segments according to different Tajweed rules based on the acoustic patterns in the recitation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cfe2158",
      "metadata": {
        "id": "6cfe2158",
        "papermill": {
          "duration": 0.003444,
          "end_time": "2025-07-26T12:22:33.510457",
          "exception": false,
          "start_time": "2025-07-26T12:22:33.507013",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## üéõÔ∏è Configuration & Imports\n",
        "\n",
        "Below we set all hyperparameters in one place (`CONFIG`), and import the libraries you'll need.\n",
        "\n",
        "**Key Parameters Explained:**\n",
        "- **`SAMPLE_RATE`**: 16kHz - Standard rate for speech processing, balancing quality and computational efficiency\n",
        "- **`MAX_SEC`**: 4.0 seconds - Fixed duration for all audio clips to ensure consistent input size\n",
        "- **`MAX_LEN`**: 64,000 samples - Calculated as SAMPLE_RATE √ó MAX_SEC\n",
        "- **`BATCH`**, **`EPOCHS`**, **`LR`**: Training hyperparameters optimized for audio classification\n",
        "- **`K_FOLDS`**: 5-fold cross-validation for robust model evaluation\n",
        "\n",
        "We use:\n",
        "- **torchaudio** for audio I/O and preprocessing\n",
        "- **torchmetrics** for F1-score calculation (important for potentially imbalanced classes)\n",
        "- **tqdm** for training progress visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5-OcTOKj38E9",
      "metadata": {
        "id": "5-OcTOKj38E9"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "# run this if you are working on colab(disgusting ü§Æ)\n",
        "!pip install torchmetrics\n",
        "\n",
        "clear_output()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8pfhL_gF3-T2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pfhL_gF3-T2",
        "outputId": "13b401c4-e196-4ff8-8da3-3351638b1894"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"mohammad2012191/tajweed-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38e3a21e",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-07-26T12:22:33.517120Z",
          "iopub.status.busy": "2025-07-26T12:22:33.516848Z",
          "iopub.status.idle": "2025-07-26T12:22:49.837103Z",
          "shell.execute_reply": "2025-07-26T12:22:49.836326Z"
        },
        "id": "38e3a21e",
        "outputId": "1f0ca6a3-278d-44ce-947a-b77f883da4ae",
        "papermill": {
          "duration": 16.325042,
          "end_time": "2025-07-26T12:22:49.838280",
          "exception": false,
          "start_time": "2025-07-26T12:22:33.513238",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import random, os\n",
        "import numpy as np, pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn, torch.nn.functional as F\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from torchmetrics.classification import F1Score\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# üé≤ Set random seeds for reproducibility across all libraries\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "# üéµ Audio processing parameters\n",
        "SAMPLE_RATE = 16_000  # 16kHz sampling rate - optimal for speech\n",
        "MAX_SEC     = 4.0     # Fixed duration for all audio clips\n",
        "MAX_LEN     = int(SAMPLE_RATE * MAX_SEC)  # 64,000 samples per clip\n",
        "\n",
        "# üèãÔ∏è Training hyperparameters\n",
        "BATCH    = 32   # Batch size - balance between memory usage and gradient stability\n",
        "EPOCHS   = 3   # Number of training epochs\n",
        "LR       = 3e-4 # Learning rate - conservative value for stable training\n",
        "K_FOLDS  = 5    # Number of cross-validation folds\n",
        "\n",
        "# üìÅ Dataset paths\n",
        "WORK_DIR  = Path(path)\n",
        "TRAIN_CSV = WORK_DIR/\"train.csv\"  # CSV with audio file IDs and labels\n",
        "TRAIN_DIR = WORK_DIR/\"train\"      # Directory containing .wav files\n",
        "\n",
        "# üñ•Ô∏è Device configuration - use GPU if available for faster training\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"üñ•Ô∏è Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c986c93b",
      "metadata": {
        "id": "c986c93b",
        "papermill": {
          "duration": 0.002512,
          "end_time": "2025-07-26T12:22:49.843826",
          "exception": false,
          "start_time": "2025-07-26T12:22:49.841314",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## üîç Exploratory Data Analysis (EDA)\n",
        "\n",
        "Understanding your dataset is crucial for successful audio classification. Let's explore:\n",
        "\n",
        "1. **Reciter Distribution**: Different speakers may have varying vocal characteristics\n",
        "2. **Tajweed Rule Distribution**: Class imbalance can affect model performance\n",
        "3. **Audio Characteristics**: Understanding the diversity in our audio data\n",
        "\n",
        "**Why EDA matters for audio:**\n",
        "- **Speaker Variability**: Different reciters have unique vocal characteristics\n",
        "- **Class Imbalance**: Some Tajweed rules might be more common than others\n",
        "- **Audio Quality**: Consistent preprocessing ensures fair comparison across samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51e18bd0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "execution": {
          "iopub.execute_input": "2025-07-26T12:22:49.850023Z",
          "iopub.status.busy": "2025-07-26T12:22:49.849620Z",
          "iopub.status.idle": "2025-07-26T12:22:50.392758Z",
          "shell.execute_reply": "2025-07-26T12:22:50.392023Z"
        },
        "id": "51e18bd0",
        "outputId": "cc1e5eb2-d293-452f-b8f4-0fbc2415cbbd",
        "papermill": {
          "duration": 0.547543,
          "end_time": "2025-07-26T12:22:50.393948",
          "exception": false,
          "start_time": "2025-07-26T12:22:49.846405",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# üìä Load training data and perform exploratory analysis\n",
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "\n",
        "print(f\"üìà Dataset Overview:\")\n",
        "print(f\"Total samples: {len(train_df):,}\")\n",
        "print(f\"Number of reciters: {train_df.sheikh_name.nunique()}\")\n",
        "print(f\"Number of Tajweed rules: {train_df.label_name.nunique()}\")\n",
        "print(f\"\\nüìã Tajweed rules: {sorted(train_df.label_name.unique())}\")\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "reciter_counts = train_df.sheikh_name.value_counts()\n",
        "reciter_counts.plot.bar(color='skyblue', alpha=0.8)\n",
        "plt.title(\"Distribution of Reciters\", fontsize=14, pad=15)\n",
        "plt.xlabel(\"Reciter (Sheikh)\")\n",
        "plt.ylabel(\"Number of Audio Samples\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "rule_counts = train_df.label_name.value_counts()\n",
        "rule_counts.plot.bar(color='lightcoral', alpha=0.8)\n",
        "plt.title(\"Distribution of Tajweed Rules\", fontsize=14, pad=15)\n",
        "plt.xlabel(\"Tajweed Rule\")\n",
        "plt.ylabel(\"Number of Audio Samples\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4305e33",
      "metadata": {
        "id": "b4305e33",
        "papermill": {
          "duration": 0.003509,
          "end_time": "2025-07-26T12:22:50.401839",
          "exception": false,
          "start_time": "2025-07-26T12:22:50.398330",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## üéôÔ∏è Dataset & Audio Preprocessing\n",
        "\n",
        "Our PyTorch Dataset class handles the complete audio preprocessing pipeline:\n",
        "\n",
        "### üîπ **Audio Processing Steps:**\n",
        "1. **Loading**: Read `.wav` files using torchaudio\n",
        "2. **Resampling**: Ensure consistent 16kHz sampling rate\n",
        "3. **Mono Conversion**: Convert stereo to mono for simpler processing\n",
        "4. **Length Normalization**: Pad short clips, trim long clips to exactly 4 seconds\n",
        "5. **Augmentation**: Apply random transformations during training\n",
        "\n",
        "### üîπ **Audio Augmentations Explained:**\n",
        "- **Volume Adjustment**: ¬±6dB gain changes simulate different recording conditions\n",
        "- **Frequency Masking**: Random frequency bands are masked to improve robustness\n",
        "- **Time Masking**: Random time segments are masked to prevent overfitting\n",
        "\n",
        "These augmentations are inspired by SpecAugment but adapted for raw waveforms, helping the model generalize better to unseen audio conditions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0fc0908",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-07-26T12:22:50.409798Z",
          "iopub.status.busy": "2025-07-26T12:22:50.409556Z",
          "iopub.status.idle": "2025-07-26T12:22:50.414337Z",
          "shell.execute_reply": "2025-07-26T12:22:50.413787Z"
        },
        "id": "a0fc0908",
        "outputId": "53d405a9-8bce-489c-cce6-b9d71efbc0f9",
        "papermill": {
          "duration": 0.010007,
          "end_time": "2025-07-26T12:22:50.415351",
          "exception": false,
          "start_time": "2025-07-26T12:22:50.405344",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# üè∑Ô∏è Encode text labels to integers for model training\n",
        "# LabelEncoder converts string labels to consecutive integers (0, 1, 2, ...)\n",
        "le = LabelEncoder().fit(train_df[\"label_name\"])\n",
        "train_df[\"y\"] = le.transform(train_df[\"label_name\"])\n",
        "\n",
        "print(f\"üè∑Ô∏è Label Encoding Mapping:\")\n",
        "for i, label in enumerate(le.classes_):\n",
        "    count = (train_df[\"y\"] == i).sum()\n",
        "    print(f\"  {i}: {label} ({count:,} samples)\")\n",
        "\n",
        "print(f\"\\n‚úÖ Labels encoded successfully! Shape: {train_df['y'].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a62e0d0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-26T12:22:50.423363Z",
          "iopub.status.busy": "2025-07-26T12:22:50.423153Z",
          "iopub.status.idle": "2025-07-26T12:22:50.432315Z",
          "shell.execute_reply": "2025-07-26T12:22:50.431631Z"
        },
        "id": "0a62e0d0",
        "papermill": {
          "duration": 0.014435,
          "end_time": "2025-07-26T12:22:50.433351",
          "exception": false,
          "start_time": "2025-07-26T12:22:50.418916",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import random, torch\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import Compose, RandomApply\n",
        "from torchaudio.transforms import Vol, FrequencyMasking, TimeMasking\n",
        "\n",
        "# üéµ Define audio augmentation pipeline\n",
        "# These augmentations help the model generalize to different recording conditions\n",
        "audio_aug = Compose([\n",
        "    # üîä Volume augmentation: ¬±6dB gain variation (50% probability)\n",
        "    # Simulates different microphone distances and recording levels\n",
        "    RandomApply([Vol(gain=6.0, gain_type='db')], p=0.5),\n",
        "\n",
        "    # üéº Frequency masking: Hide random frequency bands (50% probability)\n",
        "    # Helps model focus on important frequency ranges, reduces overfitting\n",
        "    RandomApply([FrequencyMasking(freq_mask_param=30)], p=0.5),\n",
        "\n",
        "    # ‚è∞ Time masking: Hide random time segments (50% probability)\n",
        "    # Simulates brief audio dropouts, improves temporal robustness\n",
        "    RandomApply([TimeMasking(time_mask_param=80)], p=0.5),\n",
        "])\n",
        "\n",
        "class TajweedDataset(Dataset):\n",
        "    \"\"\"\n",
        "    üéôÔ∏è PyTorch Dataset for Tajweed Audio Classification\n",
        "\n",
        "    This dataset handles loading, preprocessing, and augmentation of audio files\n",
        "    for training a 1D CNN on Tajweed rule classification.\n",
        "\n",
        "    Key Features:\n",
        "    - Automatic resampling to target sample rate\n",
        "    - Length normalization (pad/trim to fixed duration)\n",
        "    - Optional augmentations for improved generalization\n",
        "    - Flexible return format for training vs inference\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame with 'id' column and optionally 'y' (labels)\n",
        "        folder (str/Path): Directory containing audio files named as '{id}.wav'\n",
        "        transforms (callable): Audio augmentation pipeline (None for inference)\n",
        "        return_id (bool): If True, return (audio, id) for inference mode\n",
        "                         If False, return (audio, label) for training mode\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, df, folder, transforms=None, return_id=False):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.folder = Path(folder)\n",
        "        self.transforms = transforms\n",
        "        self.return_id = return_id\n",
        "\n",
        "        print(f\"üìÅ Dataset initialized: {len(self.df)} samples from {self.folder}\")\n",
        "\n",
        "    def _pad_trim(self, wav: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        üîß Normalize audio length to exactly MAX_LEN samples\n",
        "\n",
        "        - If too short: Zero-pad at the end\n",
        "        - If too long: Center-crop to target length\n",
        "\n",
        "        This ensures all inputs have identical shape for batch processing.\n",
        "        \"\"\"\n",
        "        current_len = wav.size(1)\n",
        "\n",
        "        if current_len < MAX_LEN:\n",
        "            # Pad with zeros at the end\n",
        "            padding = MAX_LEN - current_len\n",
        "            return F.pad(wav, (0, padding))\n",
        "        elif current_len > MAX_LEN:\n",
        "            # Center crop to target length\n",
        "            start_idx = (current_len - MAX_LEN) // 2\n",
        "            return wav[:, start_idx:start_idx + MAX_LEN]\n",
        "        else:\n",
        "            return wav\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        # üìÇ Load audio file\n",
        "        audio_path = self.folder / f\"{row['id']}.wav\"\n",
        "\n",
        "        try:\n",
        "            # Load waveform and sample rate\n",
        "            wav, sr = torchaudio.load(str(audio_path))\n",
        "\n",
        "            # üîÑ Resample if necessary to ensure consistent sample rate\n",
        "            if sr != SAMPLE_RATE:\n",
        "                wav = torchaudio.functional.resample(wav, sr, SAMPLE_RATE)\n",
        "\n",
        "            # üéõÔ∏è Apply augmentations if provided (only during training)\n",
        "            if self.transforms is not None:\n",
        "                wav = self.transforms(wav)\n",
        "\n",
        "            # üéµ Convert to mono by averaging channels\n",
        "            wav = wav.mean(dim=0, keepdim=True)\n",
        "\n",
        "            # üìè Normalize length to exactly MAX_LEN samples\n",
        "            wav = self._pad_trim(wav)\n",
        "\n",
        "            # üîÑ Return format depends on mode\n",
        "            if self.return_id:\n",
        "                # Inference mode: return audio and file ID\n",
        "                return wav, row[\"id\"]\n",
        "            else:\n",
        "                # Training mode: return audio and encoded label\n",
        "                return wav, torch.tensor(row[\"y\"], dtype=torch.long)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading {audio_path}: {e}\")\n",
        "            # Return zeros if file loading fails\n",
        "            zero_wav = torch.zeros(1, MAX_LEN)\n",
        "            if self.return_id:\n",
        "                return zero_wav, row[\"id\"]\n",
        "            else:\n",
        "                return zero_wav, torch.tensor(0, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce689a5e",
      "metadata": {
        "id": "ce689a5e",
        "papermill": {
          "duration": 0.003387,
          "end_time": "2025-07-26T12:22:50.440248",
          "exception": false,
          "start_time": "2025-07-26T12:22:50.436861",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## üîä Audio Waveform Visualization\n",
        "\n",
        "Visualizing raw audio waveforms helps us understand:\n",
        "- **Amplitude Patterns**: How loud different segments are\n",
        "- **Temporal Structure**: How audio evolves over time  \n",
        "- **Variability**: Differences between samples and Tajweed rules\n",
        "- **Quality Check**: Ensuring our preprocessing works correctly\n",
        "\n",
        "Unlike spectrograms which show frequency content, raw waveforms display the actual signal our 1D CNN will process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f5d975d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "execution": {
          "iopub.execute_input": "2025-07-26T12:22:50.447892Z",
          "iopub.status.busy": "2025-07-26T12:22:50.447685Z",
          "iopub.status.idle": "2025-07-26T12:22:51.044603Z",
          "shell.execute_reply": "2025-07-26T12:22:51.043857Z"
        },
        "id": "4f5d975d",
        "outputId": "5d6c5180-dc4d-465d-e20c-5fd4954e5dbb",
        "papermill": {
          "duration": 0.60209,
          "end_time": "2025-07-26T12:22:51.045844",
          "exception": false,
          "start_time": "2025-07-26T12:22:50.443754",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# üñºÔ∏è Visualize sample waveforms to understand our data\n",
        "\n",
        "# Select 4 random samples with different labels for diversity\n",
        "sample_data = train_df.groupby('label_name').head(1).head(4)\n",
        "sample_dataset = TajweedDataset(sample_data, TRAIN_DIR, transforms=None)\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "for i in range(len(sample_dataset)):\n",
        "    wav, label_idx = sample_dataset[i]\n",
        "    row = sample_dataset.df.iloc[i]\n",
        "\n",
        "    file_id = row[\"id\"]\n",
        "    label_name = row[\"label_name\"]\n",
        "    sheikh_name = row[\"sheikh_name\"]\n",
        "\n",
        "    # Create time axis for x-axis (in seconds)\n",
        "    time_axis = np.linspace(0, MAX_SEC, len(wav.squeeze()))\n",
        "\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    plt.plot(time_axis, wav.squeeze().numpy(), color='steelblue', alpha=0.8)\n",
        "    plt.title(f\"{file_id}\\nRule: {label_name}\\nSheikh: {sheikh_name}\",\n",
        "              fontsize=10, pad=10)\n",
        "    plt.xlabel(\"Time (seconds)\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.xlim(0, MAX_SEC)\n",
        "\n",
        "    # Add some statistics\n",
        "    max_amp = wav.abs().max().item()\n",
        "    mean_amp = wav.abs().mean().item()\n",
        "    plt.text(0.02, 0.95, f\"Max: {max_amp:.3f}\\nMean: {mean_amp:.3f}\",\n",
        "             transform=plt.gca().transAxes, fontsize=8,\n",
        "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
        "\n",
        "plt.suptitle(\"üéµ Sample Tajweed Audio Waveforms\", fontsize=16, y=0.98)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03f6c99b",
      "metadata": {
        "id": "03f6c99b",
        "papermill": {
          "duration": 0.004413,
          "end_time": "2025-07-26T12:22:51.055519",
          "exception": false,
          "start_time": "2025-07-26T12:22:51.051106",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## üèóÔ∏è 1D CNN Architecture\n",
        "\n",
        "Our 1D CNN is specifically designed for audio classification with the following key components:\n",
        "\n",
        "### üîπ **Architecture Design Principles:**\n",
        "\n",
        "1. **Progressive Downsampling**: Each conv layer reduces temporal resolution while increasing channel depth\n",
        "2. **Batch Normalization**: Stabilizes training and accelerates convergence  \n",
        "3. **ReLU Activation**: Introduces non-linearity for complex pattern learning\n",
        "4. **Global Average Pooling**: Reduces overfitting compared to fully connected layers\n",
        "5. **Moderate Depth**: 4 conv layers balance capacity with computational efficiency\n",
        "\n",
        "### üîπ **Layer-by-Layer Breakdown:**\n",
        "- **Input**: (batch, 1, 64000) - Raw mono audio waveform\n",
        "- **Conv1**: 1‚Üí32 channels, kernel=11, stride=2 ‚Üí (batch, 32, 32000)\n",
        "- **Conv2**: 32‚Üí64 channels, kernel=11, stride=2 ‚Üí (batch, 64, 16000)  \n",
        "- **Conv3**: 64‚Üí128 channels, kernel=11, stride=2 ‚Üí (batch, 128, 8000)\n",
        "- **Conv4**: 128‚Üí256 channels, kernel=11, stride=2 ‚Üí (batch, 256, 4000)\n",
        "- **GlobalAvgPool**: ‚Üí (batch, 256, 1)\n",
        "- **Classifier**: 256 ‚Üí num_classes\n",
        "\n",
        "### üîπ **Why These Choices:**\n",
        "- **Kernel Size 11**: Captures meaningful temporal patterns in audio\n",
        "- **Stride 2**: Aggressive downsampling reduces computation while preserving information\n",
        "- **Channel Progression**: 1‚Üí32‚Üí64‚Üí128‚Üí256 follows common CNN scaling patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d73a81d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-07-26T12:22:51.066461Z",
          "iopub.status.busy": "2025-07-26T12:22:51.065771Z",
          "iopub.status.idle": "2025-07-26T12:22:51.071334Z",
          "shell.execute_reply": "2025-07-26T12:22:51.070628Z"
        },
        "id": "d73a81d1",
        "outputId": "efec37c3-cedc-49e9-d207-4266e3c08c11",
        "papermill": {
          "duration": 0.011789,
          "end_time": "2025-07-26T12:22:51.072531",
          "exception": false,
          "start_time": "2025-07-26T12:22:51.060742",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class CNN1D(nn.Module):\n",
        "    \"\"\"\n",
        "    üèóÔ∏è 1D Convolutional Neural Network for Audio Classification\n",
        "\n",
        "    This architecture processes raw audio waveforms using temporal convolutions\n",
        "    to learn hierarchical features for Tajweed rule classification.\n",
        "\n",
        "    Architecture Flow:\n",
        "    Raw Audio ‚Üí Conv Blocks ‚Üí Global Pooling ‚Üí Classification\n",
        "\n",
        "    Key Features:\n",
        "    - Direct waveform processing (no spectrograms needed)\n",
        "    - Progressive feature extraction through conv layers\n",
        "    - Batch normalization for stable training\n",
        "    - Global average pooling for translation invariance\n",
        "    - Compact classifier head\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        # üîß Feature extraction backbone\n",
        "        self.features = nn.Sequential(\n",
        "            # üîµ Block 1: 1 ‚Üí 32 channels (kernel=11, stride=2)\n",
        "            # Captures basic temporal patterns in raw audio\n",
        "            nn.Conv1d(in_channels=1, out_channels=32, kernel_size=11, stride=2, padding=5),\n",
        "            nn.BatchNorm1d(32),  # Normalize for stable training\n",
        "            nn.ReLU(inplace=True),  # Non-linear activation\n",
        "\n",
        "            # üü° Block 2: 32 ‚Üí 64 channels\n",
        "            # Learns combinations of basic patterns\n",
        "            nn.Conv1d(32, 64, kernel_size=11, stride=2, padding=5),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # üü† Block 3: 64 ‚Üí 128 channels\n",
        "            # Captures mid-level temporal structures\n",
        "            nn.Conv1d(64, 128, kernel_size=11, stride=2, padding=5),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # üî¥ Block 4: 128 ‚Üí 256 channels\n",
        "            # High-level feature representation\n",
        "            nn.Conv1d(128, 256, kernel_size=11, stride=2, padding=5),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # üåê Global Average Pooling\n",
        "            # Reduces each channel to single value, provides translation invariance\n",
        "            nn.AdaptiveAvgPool1d(1)\n",
        "        )\n",
        "\n",
        "        # üéØ Classification head\n",
        "        self.classifier = nn.Linear(256, n_classes)\n",
        "\n",
        "        # üìä Print model info\n",
        "        total_params = sum(p.numel() for p in self.parameters())\n",
        "        print(f\"üèóÔ∏è Model created with {total_params:,} parameters\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the 1D CNN\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, 1, sequence_length)\n",
        "\n",
        "        Returns:\n",
        "            Logits tensor of shape (batch_size, n_classes)\n",
        "        \"\"\"\n",
        "        # Extract features through conv blocks\n",
        "        features = self.features(x)  # (batch, 256, 1)\n",
        "\n",
        "        # Remove length dimension and apply classifier\n",
        "        features = features.squeeze(-1)  # (batch, 256)\n",
        "        logits = self.classifier(features)  # (batch, n_classes)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# üß™ Test model with dummy input to verify shapes\n",
        "n_classes = len(train_df.label_name.unique())\n",
        "model = CNN1D(n_classes)\n",
        "\n",
        "# Create dummy input batch\n",
        "dummy_input = torch.randn(2, 1, MAX_LEN)  # 2 samples for testing\n",
        "with torch.no_grad():\n",
        "    dummy_output = model(dummy_input)\n",
        "\n",
        "print(f\"‚úÖ Model test successful!\")\n",
        "print(f\"üì• Input shape: {dummy_input.shape}\")\n",
        "print(f\"üì§ Output shape: {dummy_output.shape}\")\n",
        "print(f\"üéØ Number of classes: {n_classes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d55f4f8c",
      "metadata": {
        "id": "d55f4f8c",
        "papermill": {
          "duration": 0.00441,
          "end_time": "2025-07-26T12:22:51.081373",
          "exception": false,
          "start_time": "2025-07-26T12:22:51.076963",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## üîÑ Training with K-Fold Cross-Validation\n",
        "\n",
        "We use **5-fold stratified cross-validation** to ensure robust model evaluation:\n",
        "\n",
        "### üîπ **Why K-Fold CV for Audio?**\n",
        "1. **Robust Evaluation**: Reduces dependence on specific train/test splits\n",
        "2. **Speaker Independence**: Helps ensure model generalizes across different reciters\n",
        "3. **Class Balance**: Stratified splitting maintains label distribution in each fold\n",
        "4. **Statistical Significance**: Multiple folds provide confidence intervals\n",
        "\n",
        "### üîπ **Training Process per Fold:**\n",
        "1. **Data Split**: 80% train, 20% validation (stratified by Tajweed rule)\n",
        "2. **Augmentation**: Apply audio augmentations only to training data\n",
        "3. **Optimization**: Adam optimizer with cosine annealing learning rate schedule\n",
        "4. **Evaluation**: Macro-F1 score (equal weight to all classes)\n",
        "5. **Checkpointing**: Save best model based on validation F1\n",
        "\n",
        "### üîπ **Key Metrics:**\n",
        "- **Macro-F1**: Average F1 across all classes (handles class imbalance)\n",
        "- **Loss Tracking**: Monitor training convergence\n",
        "- **Best Model**: Saved when validation F1 improves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1661879c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-07-26T12:22:51.091301Z",
          "iopub.status.busy": "2025-07-26T12:22:51.091095Z",
          "iopub.status.idle": "2025-07-26T12:35:02.681425Z",
          "shell.execute_reply": "2025-07-26T12:35:02.680147Z"
        },
        "id": "1661879c",
        "outputId": "536ebe10-6717-4df1-d366-a491f134b70f",
        "papermill": {
          "duration": 731.596924,
          "end_time": "2025-07-26T12:35:02.682762",
          "exception": false,
          "start_time": "2025-07-26T12:22:51.085838",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Initialize cross-validation splitter\n",
        "skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=SEED)\n",
        "val_scores = []  # Store validation F1 scores for each fold\n",
        "\n",
        "# üìä Track training progress\n",
        "all_fold_results = []\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(skf.split(train_df, train_df.label_name), 1):\n",
        "\n",
        "\n",
        "\n",
        "    # Training loader with augmentations\n",
        "    tr_dataset = TajweedDataset(train_df.iloc[tr_idx], TRAIN_DIR,\n",
        "                               transforms=audio_aug, return_id=False)\n",
        "    tr_dl = DataLoader(tr_dataset, batch_size=BATCH, shuffle=True,\n",
        "                      num_workers=2, pin_memory=True)\n",
        "\n",
        "    # Validation loader without augmentations\n",
        "    va_dataset = TajweedDataset(train_df.iloc[va_idx], TRAIN_DIR,\n",
        "                               transforms=None, return_id=False)\n",
        "    va_dl = DataLoader(va_dataset, batch_size=BATCH, shuffle=False,\n",
        "                      num_workers=2, pin_memory=True)\n",
        "\n",
        "    # üèóÔ∏è Initialize model and training components\n",
        "    model = CNN1D(n_classes=len(train_df.label_name.unique())).to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # üìä F1 score metric for evaluation (macro averaging for balanced evaluation)\n",
        "    f1_metric = F1Score(task=\"multiclass\",\n",
        "                       num_classes=len(train_df.label_name.unique()),\n",
        "                       average=\"macro\").to(DEVICE)\n",
        "\n",
        "    # üèÜ Track best performance\n",
        "    best_val_f1 = 0.0\n",
        "    fold_train_losses = []\n",
        "    fold_val_f1s = []\n",
        "\n",
        "    # üèãÔ∏è Training loop for current fold\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "\n",
        "        # üî• TRAINING PHASE\n",
        "        model.train()\n",
        "        epoch_train_losses = []\n",
        "\n",
        "        train_pbar = tqdm(tr_dl, desc=f\"Fold {fold} Epoch {epoch:2d}/{EPOCHS} [Train]\")\n",
        "        for batch_idx, (audio_batch, label_batch) in enumerate(train_pbar):\n",
        "            # Move to device\n",
        "            audio_batch = audio_batch.to(DEVICE, non_blocking=True)\n",
        "            label_batch = label_batch.to(DEVICE, non_blocking=True)\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(audio_batch)\n",
        "            loss = criterion(logits, label_batch)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track loss\n",
        "            batch_loss = loss.item()\n",
        "            epoch_train_losses.append(batch_loss)\n",
        "            fold_train_losses.append(batch_loss)\n",
        "\n",
        "            # Update progress bar\n",
        "            train_pbar.set_postfix({\n",
        "                'Loss': f'{batch_loss:.4f}',\n",
        "                'LR': f'{optimizer.param_groups[0][\"lr\"]:.2e}'\n",
        "            })\n",
        "\n",
        "        # üìä VALIDATION PHASE\n",
        "        def evaluate_model(dataloader, metric):\n",
        "            \"\"\"Helper function to evaluate model on given dataloader\"\"\"\n",
        "            model.eval()\n",
        "            metric.reset()\n",
        "            total_loss = 0.0\n",
        "            num_batches = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for audio_batch, label_batch in dataloader:\n",
        "                    audio_batch = audio_batch.to(DEVICE, non_blocking=True)\n",
        "                    label_batch = label_batch.to(DEVICE, non_blocking=True)\n",
        "\n",
        "                    # Forward pass\n",
        "                    logits = model(audio_batch)\n",
        "                    loss = criterion(logits, label_batch)\n",
        "\n",
        "                    # Update metrics\n",
        "                    predictions = logits.softmax(dim=1)\n",
        "                    metric.update(predictions, label_batch)\n",
        "                    total_loss += loss.item()\n",
        "                    num_batches += 1\n",
        "\n",
        "            return metric.compute().item(), total_loss / num_batches\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        val_f1, val_loss = evaluate_model(va_dl, f1_metric)\n",
        "        fold_val_f1s.append(val_f1)\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        # üèÜ Save best model\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            checkpoint_path = f\"fold{fold}_best.pt\"\n",
        "            torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'epoch': epoch,\n",
        "                'val_f1': val_f1,\n",
        "                'fold': fold\n",
        "            }, checkpoint_path)\n",
        "            print(f\"üèÜ New best model saved! Val F1: {val_f1:.4f}\")\n",
        "\n",
        "        # üìä Epoch summary\n",
        "        avg_train_loss = np.mean(epoch_train_losses)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "\n",
        "\n",
        "    # üìà Fold summary\n",
        "    val_scores.append(best_val_f1)\n",
        "    fold_results = {\n",
        "        'fold': fold,\n",
        "        'best_val_f1': best_val_f1,\n",
        "        'train_losses': fold_train_losses,\n",
        "        'val_f1s': fold_val_f1s\n",
        "    }\n",
        "    all_fold_results.append(fold_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jKG29JUaDiO2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKG29JUaDiO2",
        "outputId": "497ebb3b-1045-45ed-d0e7-ac62b75a095a"
      },
      "outputs": [],
      "source": [
        "print(f\"üìä Individual Fold Results:\")\n",
        "for i, score in enumerate(val_scores, 1):\n",
        "    print(f\"  Fold {i}: {score:.4f}\")\n",
        "\n",
        "mean_cv_score = np.mean(val_scores)\n",
        "std_cv_score = np.std(val_scores)\n",
        "\n",
        "print(f\"\\nüéØ Final Cross-Validation Results:\")\n",
        "print(f\"üìà Mean F1 Score: {mean_cv_score:.4f} ¬± {std_cv_score:.4f}\")\n",
        "print(f\"üìä Best Single Fold: {max(val_scores):.4f}\")\n",
        "print(f\"üìâ Worst Single Fold: {min(val_scores):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "742e6761",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "742e6761",
        "outputId": "7140a232-3d32-44d0-fcfa-cce1536e9640"
      },
      "outputs": [],
      "source": [
        "# üé® Visualize training progress\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Plot 1: F1 scores across folds\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.bar(range(1, K_FOLDS + 1), val_scores, color='skyblue', alpha=0.8)\n",
        "plt.axhline(mean_cv_score, color='red', linestyle='--', alpha=0.8,\n",
        "           label=f'Mean: {mean_cv_score:.4f}')\n",
        "plt.title('Validation F1 Scores by Fold')\n",
        "plt.xlabel('Fold')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.legend()\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Plot 2: Training loss progression (last fold)\n",
        "plt.subplot(1, 3, 2)\n",
        "if all_fold_results:\n",
        "    last_fold_losses = all_fold_results[-1]['train_losses']\n",
        "    # Smooth the losses for better visualization\n",
        "    window_size = max(1, len(last_fold_losses) // 50)\n",
        "    if len(last_fold_losses) > window_size:\n",
        "        smoothed_losses = np.convolve(last_fold_losses,\n",
        "                                    np.ones(window_size)/window_size, mode='valid')\n",
        "        plt.plot(smoothed_losses, color='orange', alpha=0.8)\n",
        "    else:\n",
        "        plt.plot(last_fold_losses, color='orange', alpha=0.8)\n",
        "plt.title(f'Training Loss (Fold {K_FOLDS})')\n",
        "plt.xlabel('Training Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Validation F1 progression (last fold)\n",
        "plt.subplot(1, 3, 3)\n",
        "if all_fold_results:\n",
        "    last_fold_val_f1s = all_fold_results[-1]['val_f1s']\n",
        "    plt.plot(range(1, len(last_fold_val_f1s) + 1), last_fold_val_f1s,\n",
        "             color='green', marker='o', alpha=0.8)\n",
        "plt.title(f'Validation F1 (Fold {K_FOLDS})')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25d34816",
      "metadata": {
        "id": "25d34816"
      },
      "source": [
        "## üéä Training Results & Model Performance\n",
        "\n",
        "### üîπ **Understanding the Results:**\n",
        "\n",
        "**Cross-Validation F1 Scores**: Our model achieved consistent performance across all folds, indicating:\n",
        "- **Robustness**: Model generalizes well to different data splits\n",
        "- **Stability**: Low variance between folds suggests reliable training\n",
        "- **Balance**: Macro-F1 ensures all Tajweed rules are learned equally\n",
        "\n",
        "### üîπ **What Makes This Approach Effective:**\n",
        "\n",
        "1. **Direct Waveform Processing**: No feature engineering required - the CNN learns optimal representations\n",
        "2. **Temporal Pattern Recognition**: 1D convolutions capture time-dependent acoustic patterns\n",
        "3. **Data Augmentation**: Audio transformations improve generalization to new recording conditions\n",
        "4. **Cross-Validation**: Ensures robust evaluation across different speaker combinations\n",
        "\n",
        "### üîπ **Next Steps for Improvement:**\n",
        "\n",
        "üöÄ **Model Enhancements:**\n",
        "- **Deeper Networks**: Add more convolutional layers\n",
        "- **Residual Connections**: Skip connections for better gradient flow\n",
        "- **Attention Mechanisms**: Focus on important temporal regions\n",
        "- **Multi-Scale Processing**: Parallel paths with different kernel sizes\n",
        "\n",
        "üéµ **Audio Processing:**\n",
        "- **Longer Clips**: Process 8-10 second segments for more context\n",
        "- **Mel-Spectrograms**: Combine with frequency-domain features\n",
        "- **Advanced Augmentations**: Pitch shifting, speed changes, noise addition\n",
        "\n",
        "üìä **Training Strategies:**\n",
        "- **Transfer Learning**: Pre-train on larger speech datasets\n",
        "- **Ensemble Methods**: Combine predictions from multiple folds\n",
        "- **Class Balancing**: Address any remaining class imbalance issues\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iWso5K2YFXph",
      "metadata": {
        "id": "iWso5K2YFXph"
      },
      "source": [
        "# Contributed by: Mohamed Eltayeb (He did everything) - Edited by: Ali Habibullah (I added comments and markdowns üò∂‚Äçüå´Ô∏è)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "databundleVersionId": 13177377,
          "sourceId": 108777,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31090,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 793.642217,
      "end_time": "2025-07-26T12:35:43.122862",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-07-26T12:22:29.480645",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
